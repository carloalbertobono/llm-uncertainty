{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ab59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e332ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"turl_test_2k_prompts_50.jsonl\"\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "model_name = \"osunlp/TableLlama\"\n",
    "config = transformers.AutoConfig.from_pretrained(model_name)\n",
    "orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=orig_ctx_len, padding_side=\"left\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d387cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt formatting\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_seg}\\n\\n### Question:\\n{question}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def generate_prompt(instruction, question, input_seg=None):\n",
    "    question += \" Answer with just a candidate, selected from the provided referent entity candidates list, and nothing else. The selected candidate must be reported verbatim from the list provided as input. Each candidate in the list is enclosed between < and > and reports [DESC] and [TYPE] information.\"\n",
    "    if input_seg:\n",
    "        return PROMPT_DICT[\"prompt_input\"].format(instruction=instruction, input_seg=input_seg, question=question)\n",
    "    else:\n",
    "        return PROMPT_DICT[\"prompt_no_input\"].format(instruction=instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "947af3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 37203968\r\n",
      "drwx------  1 bono  staff   1.0M Feb 17 08:52 \u001b[34m..\u001b[m\u001b[m\r\n",
      "-rwx------@ 1 bono  staff   4.4G Mar  7 12:23 \u001b[31mTableLlama.1548780410.pickle\u001b[m\u001b[m\r\n",
      "drwx------  1 bono  staff   1.0M Mar  7 14:45 \u001b[34m.\u001b[m\u001b[m\r\n",
      "-rwx------  1 bono  staff   4.0K Mar  7 14:46 \u001b[31m._TableLlama.1548780410.pickle\u001b[m\u001b[m\r\n",
      "-rwx------@ 1 bono  staff   4.4G Mar  8 14:55 \u001b[31mTableLlama.2573653229.pickle\u001b[m\u001b[m\r\n",
      "-rwx------  1 bono  staff   4.0K Mar  8 14:56 \u001b[31m._TableLlama.2573653229.pickle\u001b[m\u001b[m\r\n",
      "-rwx------@ 1 bono  staff   4.4G Mar  9 10:21 \u001b[31mTableLlama.1765523202.pickle\u001b[m\u001b[m\r\n",
      "-rwx------  1 bono  staff   4.0K Mar  9 10:27 \u001b[31m._TableLlama.1765523202.pickle\u001b[m\u001b[m\r\n",
      "-rwx------@ 1 bono  staff   4.4G Mar 10 08:18 \u001b[31mTableLlama.940523022.pickle\u001b[m\u001b[m\r\n",
      "-rwx------  1 bono  staff   4.0K Mar 10 08:35 \u001b[31m._TableLlama.940523022.pickle\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls -larth \"/Volumes/ssd/uncertainty/tablellama su 1800 prompts/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9d97183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TableLlama.1548780410.pickle\n",
      "TableLlama.1765523202.pickle\n",
      "TableLlama.2573653229.pickle\n",
      "TableLlama.940523022.pickle\n",
      "CPU times: user 1min 58s, sys: 49.7 s, total: 2min 48s\n",
      "Wall time: 2min 59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7204"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load processed data\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "prefix = '/Volumes/ssd/uncertainty/tablellama su 1800 prompts/'\n",
    "\n",
    "run = 0\n",
    "\n",
    "outlist = []\n",
    "for file in os.listdir(prefix):\n",
    "    if file.endswith('pickle') and not file.startswith('.'):\n",
    "        print(file)\n",
    "        with open(os.path.join(prefix, file), 'rb') as handle:\n",
    "            outlist_ = pickle.load(handle)\n",
    "           \n",
    "            for pid, o in enumerate(outlist_):\n",
    "                del o['pre_output_proba_topn']\n",
    "                del o['pre_output_proba_topk']\n",
    "                del o['pre_output_true_entropies']\n",
    "                del o['post_output_proba_topn']\n",
    "                del o['post_output_proba_topk']\n",
    "                del o['post_output_true_entropies']\n",
    "                o['run'] = run\n",
    "                o['pid'] = pid\n",
    "                outlist.append(o)\n",
    "        run += 1\n",
    "\n",
    "len(outlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6ce8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if False:\n",
    "    with open('1800.pickle', 'wb') as handle:\n",
    "        pickle.dump(outlist, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open('1800.pickle', 'rb') as handle:\n",
    "        outlist = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab98ed2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7204"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f007b83e",
   "metadata": {},
   "source": [
    "### check outputs against ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b59b037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4857 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 2.28 s, total: 1min 11s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# show results\n",
    "\n",
    "c=0\n",
    "t=0\n",
    "h=0\n",
    "\n",
    "truth = []\n",
    "\n",
    "for idx, p in enumerate(outlist):\n",
    "    print(idx, end='\\r')\n",
    "    c+=1\n",
    "    # in\n",
    "    prompt = generate_prompt(p[\"instruction\"], p[\"question\"], p[\"input\"])\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    # computed\n",
    "    # post_output_scores = p['post_output_scores']\n",
    "    post_output_sequences = p['post_output_sequences']\n",
    "    # bases\n",
    "    baseid = len(inputs[\"input_ids\"][0]) + 1\n",
    "    endid = len(post_output_sequences[0])\n",
    "    # lookout\n",
    "    generated_ids = post_output_sequences\n",
    "    generated_text = tokenizer.decode(generated_ids[0][baseid:endid], skip_special_tokens=True)\n",
    "    # print\n",
    "    # print(generated_text)\n",
    "    # print(p['output'])\n",
    "    # print('\\n')\n",
    "    #test\n",
    "    a = generated_text.lower().strip()\n",
    "    b = p['output'].lower().strip()\n",
    "    # correct\n",
    "    correct = False\n",
    "    if (a in b) or (b in a) or (b.startswith(a)) or (a.startswith(b)): \n",
    "        correct = True\n",
    "        t+=1\n",
    "    # hallucinated\n",
    "    elif a not in prompt.lower().strip(): \n",
    "        h+=1\n",
    "    # incorrect\n",
    "    else:\n",
    "        pass\n",
    "        #print(generated_text)\n",
    "        #print(p['output'])\n",
    "        #print('\\n')\n",
    "        # print(prompt)\n",
    "        # print('\\n')\n",
    "        \n",
    "    truth.append((p['run'], p['pid'], correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc34a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = pd.DataFrame(truth, columns=['run', 'pid', 'correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9840ce60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    1801\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth.groupby('pid').size().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "583aa919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='correct', ylabel='Count'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(truth.groupby('pid').correct.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52c9aeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accu = truth.groupby('pid').correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0efd3d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(344,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu[accu<1.].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62c5662a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "correct\n",
       "0.00    125\n",
       "0.75    107\n",
       "0.50     78\n",
       "0.25     34\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu[accu<1.].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "137c9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all prompts with errors plus a sample of noerrors\n",
    "err_idx = accu[accu<1.].index.tolist()\n",
    "noerr_idx = accu[accu==1.].sample(len(accu[accu<1.]), random_state=42).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f19ee6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(err_idx) & set(noerr_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80143637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "688"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_pids = set(err_idx) | set(noerr_idx)\n",
    "len(selected_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1f3417c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TableLlama.1548780410.pickle\n",
      "TableLlama.1765523202.pickle\n",
      "TableLlama.2573653229.pickle\n",
      "TableLlama.940523022.pickle\n",
      "CPU times: user 2min 20s, sys: 2min 3s, total: 4min 23s\n",
      "Wall time: 5min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2752"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load processed data\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "prefix = '/Volumes/ssd/uncertainty/tablellama su 1800 prompts/'\n",
    "\n",
    "run = 0\n",
    "\n",
    "outlist = []\n",
    "for file in os.listdir(prefix):\n",
    "    if file.endswith('pickle') and not file.startswith('.'):\n",
    "        print(file)\n",
    "        with open(os.path.join(prefix, file), 'rb') as handle:\n",
    "            outlist_ = pickle.load(handle)\n",
    "           \n",
    "            for pid, o in enumerate(outlist_):\n",
    "                if pid in selected_pids:\n",
    "                    o['run'] = run\n",
    "                    o['pid'] = pid\n",
    "                    outlist.append(o)\n",
    "        run += 1\n",
    "\n",
    "len(outlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4b4b62d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['table', 'cell', 'instruction', 'input', 'question', 'output', 'pre_output_proba_topn', 'pre_output_proba_topk', 'pre_output_true_entropies', 'post_output_sequences', 'post_output_proba_topn', 'post_output_proba_topk', 'post_output_true_entropies', 'elapsed', 'run', 'pid'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlist[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e57d940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1beafeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlist[-1]['run']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dc9317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('644.data.pickle', 'wb') as handle:\n",
    "    pickle.dump(outlist, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "749ad873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 43159720\r\n",
      "-rw-r--r--@  1 bono  staff    15M May 14  2024 turl_test_2k_prompts_50.jsonl\r\n",
      "-rw-r--r--   1 bono  staff    17B Feb 10 10:37 README.md\r\n",
      "-rwx------@  1 bono  staff   4.8K Feb 19 14:05 \u001b[31mrunner.py\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 bono  staff   137M Feb 19 16:32 gemma-2-2b-it.3892595837.pickle\r\n",
      "-rw-r--r--@  1 bono  staff   5.0K Feb 20 09:23 prompt100.py\r\n",
      "-rw-r--r--   1 bono  staff    16M Feb 20 20:07 tablellama.preprocessed.pickle\r\n",
      "-rw-r--r--@  1 bono  staff   5.2K Feb 20 21:04 prompt100tablellama.py\r\n",
      "-rw-r--r--   1 bono  staff   117M Feb 22 09:11 gemma-2-9b-it-4bit.3246737286.pickle\r\n",
      "drwxr-xr-x   4 bono  staff   128B Feb 27 07:56 \u001b[34mmirror\u001b[m\u001b[m\r\n",
      "drwxr-xr-x@ 11 bono  staff   352B Feb 27 08:19 \u001b[34m..\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 bono  staff   5.3K Mar  2 14:55 prompt100gemmalocal.py\r\n",
      "-rw-r--r--   1 bono  staff   222K Mar  4 13:37 jensension.ipynb\r\n",
      "drwxr-xr-x   4 bono  staff   128B Mar  5 20:03 \u001b[34molder tablellama\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  12 bono  staff   384B Mar  6 15:38 \u001b[34mtablellama10runs\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 bono  staff   450K Mar  6 16:03 tablellama-multipass.ipynb\r\n",
      "-rw-r--r--   1 bono  staff   450K Mar  6 16:09 tablellama-plot copy.ipynb\r\n",
      "drwxr-xr-x  16 bono  staff   512B Mar  6 17:50 \u001b[34m.git\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 bono  staff   6.2K Mar  6 19:31 prompt_wip.py\r\n",
      "-rw-rw----@  1 bono  staff   1.3M Mar  7 14:52 delme.xlsx\r\n",
      "-rw-r--r--   1 bono  staff   726K Mar  7 15:06 logits-tablellama.ipynb\r\n",
      "-rw-r--r--   1 bono  staff   779K Mar  7 15:41 tablellama-plot.ipynb\r\n",
      "-rw-r--r--@  1 bono  staff   4.4G Mar  8 13:37 TableLlama.2573653229.pickle\r\n",
      "drwxr-xr-x   2 bono  staff    64B Mar  8 14:54 \u001b[34mtest\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 bono  staff   8.0K Mar  8 14:54 .DS_Store\r\n",
      "-rw-r--r--@  1 bono  staff   4.4G Mar  9 10:21 TableLlama.1765523202.pickle\r\n",
      "-rw-r--r--@  1 bono  staff   4.4G Mar 10 08:18 TableLlama.940523022.pickle\r\n",
      "drwxr-xr-x   9 bono  staff   288B Mar 10 08:36 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 bono  staff   127M Mar 10 08:46 1800.pickle\r\n",
      "-rw-r--r--   1 bono  staff    33K Mar 10 09:00 orrors.ipynb\r\n",
      "-rw-r--r--   1 bono  staff    17K Mar 10 09:02 cohesion.ipynb\r\n",
      "drwxr-xr-x  32 bono  staff   1.0K Mar 10 09:02 \u001b[34m.\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 bono  staff   6.9G Mar 10 09:02 1800.data.pickle\r\n"
     ]
    }
   ],
   "source": [
    "!ls -larth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fbbdca",
   "metadata": {},
   "source": [
    "### DELETE ME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08531053",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './644sample/644.data.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./644sample/644.data.pickle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m      3\u001b[0m     selected \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(handle)\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './644sample/644.data.pickle'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('./644sample/644.data.pickle', 'rb') as handle:\n",
    "    selected = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ecdbbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2752"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ae2df27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2752"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "688*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3194b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.22 ms, sys: 1.83 ms, total: 5.05 ms\n",
      "Wall time: 5.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "selected_pids = set([s['pid'] for s in selected])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b627c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('selected_pids.688.pickle', 'wb') as handle:\n",
    "    pickle.dump(selected_pids, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98a006d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3810400\r\n",
      "-rw-r--r--@  1 bono  staff    15M May 14  2024 turl_test_2k_prompts_50.jsonl\r\n",
      "-rw-r--r--   1 bono  staff    17B Feb 10 10:37 README.md\r\n",
      "-rwx------@  1 bono  staff   4.8K Feb 19 14:05 \u001b[31mrunner.py\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 bono  staff   137M Feb 19 16:32 gemma-2-2b-it.3892595837.pickle\r\n",
      "-rw-r--r--@  1 bono  staff   5.0K Feb 20 09:23 prompt100.py\r\n",
      "-rw-r--r--   1 bono  staff    16M Feb 20 20:07 tablellama.preprocessed.pickle\r\n",
      "-rw-r--r--@  1 bono  staff   5.2K Feb 20 21:04 prompt100tablellama.py\r\n",
      "-rw-r--r--   1 bono  staff   117M Feb 22 09:11 gemma-2-9b-it-4bit.3246737286.pickle\r\n",
      "drwxr-xr-x   4 bono  staff   128B Feb 27 07:56 \u001b[34mmirror\u001b[m\u001b[m\r\n",
      "drwxr-xr-x@ 11 bono  staff   352B Feb 27 08:19 \u001b[34m..\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 bono  staff   5.3K Mar  2 14:55 prompt100gemmalocal.py\r\n",
      "-rw-r--r--   1 bono  staff   222K Mar  4 13:37 jensension.ipynb\r\n",
      "drwxr-xr-x   4 bono  staff   128B Mar  5 20:03 \u001b[34molder tablellama\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  12 bono  staff   384B Mar  6 15:38 \u001b[34mtablellama10runs\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 bono  staff   450K Mar  6 16:03 tablellama-multipass.ipynb\r\n",
      "-rw-r--r--   1 bono  staff   450K Mar  6 16:09 tablellama-plot copy.ipynb\r\n",
      "-rw-r--r--@  1 bono  staff   6.2K Mar  6 19:31 prompt_wip.py\r\n",
      "-rw-r--r--@  1 bono  staff   6.2K Mar  6 19:31 prompt_wip sel.py\r\n",
      "-rw-r--r--   1 bono  staff   726K Mar  7 15:06 logits-tablellama.ipynb\r\n",
      "drwxr-xr-x   2 bono  staff    64B Mar  8 14:54 \u001b[34mtest\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 bono  staff   127M Mar 10 08:46 1800.pickle\r\n",
      "-rw-rw----@  1 bono  staff   4.0M Mar 10 10:26 delme.xlsx\r\n",
      "drwxr-xr-x   9 bono  staff   288B Mar 10 16:41 \u001b[34mpdf\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  10 bono  staff   320B Mar 11 09:07 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 bono  staff   892K Mar 11 10:52 tablellama-classifier.ipynb\r\n",
      "drwxr-xr-x   4 bono  staff   128B Mar 11 15:24 \u001b[34m644sample\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 bono  staff   1.0G Mar 11 15:27 preprocessed.pickle\r\n",
      "-rw-r--r--   1 bono  staff   1.4M Mar 11 15:28 tablellama-plot.ipynb\r\n",
      "-rw-r--r--   1 bono  staff   390M Mar 11 16:16 cohesion.pickle\r\n",
      "-rw-r--r--   1 bono  staff   114K Mar 11 16:25 cohesion.ipynb\r\n",
      "drwxr-xr-x  16 bono  staff   512B Mar 11 16:36 \u001b[34m.git\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 bono  staff    10K Mar 11 17:49 .DS_Store\r\n",
      "-rw-r--r--   1 bono  staff     0B Mar 11 17:52 selected_pids.644.pickle\r\n",
      "-rw-r--r--   1 bono  staff    36K Mar 11 17:56 select_errors_from_1800.ipynb\r\n",
      "drwxr-xr-x  36 bono  staff   1.1K Mar 11 17:56 \u001b[34m.\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 bono  staff   1.9K Mar 11 18:12 selected_pids.688.pickle\r\n"
     ]
    }
   ],
   "source": [
    "!ls -larth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297c3fc",
   "metadata": {},
   "source": [
    "### debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fa3d0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/huggingface/transformers/issues/24841"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7115aaa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2deaab88d8ef47c790fa025adc42b360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "\n",
    "# run params\n",
    "temperature=1.0\n",
    "top_p=0.9\n",
    "max_new_tokens=64\n",
    "use_cache=True\n",
    "device = \"mps\"\n",
    "\n",
    "model_name = \"osunlp/TableLlama\"\n",
    "\n",
    "# load inputs\n",
    "file_path = \"turl_test_2k_prompts_50.jsonl\"\n",
    "device = torch.device(device)\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    prompts = [json.loads(line) for line in f]\n",
    "\n",
    "config = transformers.AutoConfig.from_pretrained(model_name)\n",
    "orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(device)\n",
    "model.resize_token_embeddings(32001)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=orig_ctx_len, padding_side=\"left\", use_fast=False)\n",
    "model.eval()\n",
    "\n",
    "# build prompts\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_seg}\\n\\n### Question:\\n{question}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def generate_prompt(instruction, question, input_seg=None):\n",
    "    question += \" Answer with just a candidate, selected from the provided referent entity candidates list, and nothing else. The selected candidate must be reported verbatim from the list provided as input. Each candidate in the list is enclosed between < and > and reports [DESC] and [TYPE] information.\"\n",
    "    if input_seg:\n",
    "        return PROMPT_DICT[\"prompt_input\"].format(instruction=instruction, input_seg=input_seg, question=question)\n",
    "    else:\n",
    "        return PROMPT_DICT[\"prompt_no_input\"].format(instruction=instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f866c512",
   "metadata": {},
   "source": [
    "### kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33bcc75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = prompts[840]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0df86810",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockOutputWrapper(torch.nn.Module):\n",
    "    def __init__(self, block, unembed_matrix, norm):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.unembed_matrix = unembed_matrix\n",
    "        self.norm = norm\n",
    "        self.mynorm = None\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        output = self.block(*args, **kwargs)\n",
    "        self.mynorm = self.norm(output[0])\n",
    "        # self.block_output_unembedded = self.unembed_matrix(self.norm(output[0]))\n",
    "        return output\n",
    "\n",
    "for i, layer in enumerate(model.model.layers):\n",
    "    model.model.layers[i] = BlockOutputWrapper(layer, model.lm_head, model.model.norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "59cb2639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers_kl_div_mod(pre_output, model, precomp=None):\n",
    "    softmaxed_log = torch.log_softmax(pre_output, dim=-1)\n",
    "\n",
    "    kls = []\n",
    "    for k, l in enumerate(model.model.layers):\n",
    "        # print(softmaxed_log.shape, l.shape)\n",
    "\n",
    "        # recover normalized from precomputed (if generated / hooks), or just read from network state\n",
    "        if precomp:\n",
    "            mynorm = precomp[k]\n",
    "        else:\n",
    "            mynorm = l.mynorm\n",
    "\n",
    "        l_ = l.unembed_matrix(mynorm)\n",
    "        softmaxed_l = torch.log_softmax(l_, dim=-1).cpu()\n",
    "        p = softmaxed_l[0]\n",
    "        q = softmaxed_log[0]\n",
    "        mykl = torch.nn.functional.kl_div(p, q, reduce=False, log_target=True).sum(dim=1)\n",
    "        # print(p.sum(), q.sum(), mykl.shape)\n",
    "        kls.append(mykl.detach().cpu().float().numpy())\n",
    "    return kls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c9a90632",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_fn(layer_idx):\n",
    "    def hook(module, input, output):\n",
    "        if isinstance(output, tuple) and hasattr(module, \"mynorm\"):\n",
    "            clo = module.mynorm.clone().detach()\n",
    "            print(clo.shape)\n",
    "            if clo.shape[1] > 1:\n",
    "                shpbf = clo.shape\n",
    "                clo = clo[ :, -1:, : ]\n",
    "            generated_block_outputs[layer_idx].append(clo)\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c40a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = generate_prompt(p[\"instruction\"], p[\"question\"], p[\"input\"])\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# dirty trick for caching\n",
    "prompt_ = prompt[:-1]\n",
    "inputs_ = tokenizer(prompt_, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "edf5da4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register hooks\n",
    "hooks = []\n",
    "\n",
    "for i, layer in enumerate(model.model.layers):\n",
    "    hook = layer.register_forward_hook(hook_fn(i))\n",
    "    hooks.append(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "68275f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache\n",
    "\n",
    "prompt_cache = DynamicCache()\n",
    "generated_block_outputs = {i: [] for i in range(len(model.model.layers))}\n",
    "\n",
    "pre_output = model(**inputs_, past_key_values=prompt_cache, use_cache=True)\n",
    "prompt_cache = pre_output.past_key_values\n",
    "pre_output_ = pre_output.logits.detach().cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a6251e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_block_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "722cb482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 4096])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_block_outputs[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fdbc30c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumulate norms at each generated token\n",
    "for i in generated_block_outputs:\n",
    "    generated_block_outputs[i] = ( torch.stack(generated_block_outputs[i]).squeeze(1).permute(1, 0, 2))\n",
    "generated_block_outputs = list(generated_block_outputs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3e485662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4961, -1.3359, -1.1406,  ...,  0.2471,  0.2520, -1.6484]]],\n",
       "       device='mps:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_block_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5e78cd98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_block_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dd46d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hook in hooks:\n",
    "    hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f8551d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.5 ms, sys: 208 ms, total: 228 ms\n",
      "Wall time: 328 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ooo = get_layers_kl_div_mod(pre_output.logits, model, generated_block_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c19c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ooo = get_layers_kl_div_mod(pre_output.logits, model, generated_block_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "97419849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-10.6875, -10.8125, -10.9375,  ...,  -9.1875, -10.2500, -10.4375],\n",
       "         [-15.6250, -11.9375,  -6.8125,  ..., -13.8125, -14.6250, -16.8750],\n",
       "         [-22.8750, -25.6250, -13.1875,  ..., -22.5000, -20.1250, -28.3750],\n",
       "         ...,\n",
       "         [-32.7500, -31.1250, -17.6250,  ..., -31.0000, -27.8750, -28.2500],\n",
       "         [-28.0000, -24.3750, -17.2500,  ..., -25.6250, -24.6250, -27.1250],\n",
       "         [-34.7500, -35.0000, -31.7500,  ..., -33.7500, -34.5000, -32.0000]]],\n",
       "       device='mps:0', dtype=torch.bfloat16, grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmaxed_log = torch.log_softmax(pre_output.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6e2d9954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.8 ms, sys: 529 ms, total: 572 ms\n",
      "Wall time: 728 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "aa = get_topn_dict(pre_output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f9374f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.15 s, sys: 169 ms, total: 2.32 s\n",
      "Wall time: 2.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "oo2 = get_layers_iou_div_mod(aa, model, generated_block_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d9458846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.9 ms, sys: 260 ms, total: 302 ms\n",
      "Wall time: 461 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "aa = get_topn_dict(pre_output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fd5170f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.15 s, sys: 162 ms, total: 2.31 s\n",
      "Wall time: 2.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "oo2 = get_layers_iou_div_mod(aa, model, generated_block_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5e2fc5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1072, 32001]), torch.Size([1, 1, 4096]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_output.logits.shape, generated_block_outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "588a7f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def compute_entropy_scipy(logits):\n",
    "    probabilities = torch.softmax(logits, dim=-1).detach().cpu().float().numpy()\n",
    "    entropy_values = [scipy.stats.entropy(row) for row in probabilities[0]]\n",
    "    return entropy_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "efcab437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.367176,\n",
       " 7.1573367,\n",
       " 1.7730334,\n",
       " 4.6141434,\n",
       " 1.0942366,\n",
       " 4.3756075,\n",
       " 2.4268456,\n",
       " 5.787895,\n",
       " 2.6497622,\n",
       " 4.37118,\n",
       " 0.12593651,\n",
       " 0.22030954,\n",
       " 4.589824,\n",
       " 5.7372146,\n",
       " 4.421783,\n",
       " 3.3772275,\n",
       " 3.1697783,\n",
       " 4.535813,\n",
       " 2.333696,\n",
       " 9.4758625,\n",
       " 3.2384164,\n",
       " 4.7218437,\n",
       " 2.043474,\n",
       " 1.0438491,\n",
       " 0.44690058,\n",
       " 2.1756248,\n",
       " 0.007387326,\n",
       " 1.7773242,\n",
       " 3.4982212,\n",
       " 2.1121128,\n",
       " 8.8947935,\n",
       " 2.5990195,\n",
       " 2.40515,\n",
       " 0.00030445828,\n",
       " 4.031014,\n",
       " 0.074855365,\n",
       " 0.012233714,\n",
       " 2.3385115,\n",
       " 2.3647504,\n",
       " 1.8570498,\n",
       " 0.42771012,\n",
       " 1.7364422,\n",
       " 1.8631063,\n",
       " 0.027524969,\n",
       " 0.23125592,\n",
       " 1.5359392,\n",
       " 4.3664665,\n",
       " 0.0012364393,\n",
       " 2.996109,\n",
       " 2.2359662,\n",
       " 0.00020790928,\n",
       " 0.043579713,\n",
       " 2.0784743,\n",
       " 0.89875174,\n",
       " 5.3863773,\n",
       " 4.2414403,\n",
       " 1.6508422,\n",
       " 2.3077326,\n",
       " 0.36188814,\n",
       " 3.915393,\n",
       " 1.6007016,\n",
       " 1.9049289,\n",
       " 1.8527406,\n",
       " 4.408295,\n",
       " 1.2791897,\n",
       " 1.146932,\n",
       " 4.859229,\n",
       " 0.16263445,\n",
       " 1.2868148,\n",
       " 0.02744072,\n",
       " 0.071531415,\n",
       " 0.97857016,\n",
       " 2.8513517,\n",
       " 1.4775424,\n",
       " 2.5055366,\n",
       " 0.0041745044,\n",
       " 2.7478566,\n",
       " 0.9044508,\n",
       " 3.6858509,\n",
       " 0.63584673,\n",
       " 2.379228,\n",
       " 2.2196236,\n",
       " 2.6195745,\n",
       " 3.5396664,\n",
       " 0.22987287,\n",
       " 0.89689815,\n",
       " 3.9463372,\n",
       " 0.23853919,\n",
       " 0.28618413,\n",
       " 0.25682983,\n",
       " 3.5602655,\n",
       " 0.58258843,\n",
       " 2.512694,\n",
       " 3.9406545,\n",
       " 0.16522297,\n",
       " 0.74451804,\n",
       " 3.5154457,\n",
       " 1.1941328,\n",
       " 2.0752895,\n",
       " 1.1322474,\n",
       " 2.06911,\n",
       " 0.050263375,\n",
       " 0.31719282,\n",
       " 0.04947563,\n",
       " 0.9284135,\n",
       " 0.40463364,\n",
       " 0.4432976,\n",
       " 0.02173945,\n",
       " 0.19770724,\n",
       " 2.571222,\n",
       " 2.2978256,\n",
       " 2.7455611,\n",
       " 2.5304446,\n",
       " 3.134025,\n",
       " 4.6786904,\n",
       " 2.570426,\n",
       " 2.0614798,\n",
       " 2.637663,\n",
       " 0.47798663,\n",
       " 2.026305,\n",
       " 3.1811051,\n",
       " 2.0184784,\n",
       " 0.08552499,\n",
       " 1.709643,\n",
       " 2.6346166,\n",
       " 0.8592933,\n",
       " 1.35692,\n",
       " 2.0501955,\n",
       " 0.021945527,\n",
       " 0.736,\n",
       " 0.2353367,\n",
       " 0.0007928361,\n",
       " 0.26073292,\n",
       " 0.13321754,\n",
       " 0.018918637,\n",
       " 0.021943424,\n",
       " 0.19562705,\n",
       " 0.0028673648,\n",
       " 2.033242,\n",
       " 2.3233838,\n",
       " 0.65802467,\n",
       " 0.016758699,\n",
       " 4.2367983,\n",
       " 1.5658288,\n",
       " 0.0006332735,\n",
       " 7.1792674e-05,\n",
       " 1.9843209,\n",
       " 3.660337,\n",
       " 3.3095803,\n",
       " 3.4685,\n",
       " 0.29849678,\n",
       " 0.91570884,\n",
       " 0.19258966,\n",
       " 3.688292,\n",
       " 0.69746184,\n",
       " 2.695243,\n",
       " 1.4068829,\n",
       " 2.0411007,\n",
       " 3.2584739,\n",
       " 3.541929,\n",
       " 1.4012325,\n",
       " 1.7411408,\n",
       " 2.2308002,\n",
       " 0.013036025,\n",
       " 3.1567628,\n",
       " 0.3724389,\n",
       " 3.968083,\n",
       " 0.4933771,\n",
       " 0.3380604,\n",
       " 0.017314335,\n",
       " 1.227137,\n",
       " 2.2410169,\n",
       " 1.436935,\n",
       " 0.8533001,\n",
       " 2.2470033,\n",
       " 3.6789951,\n",
       " 2.9143746,\n",
       " 2.0143085,\n",
       " 1.9129312,\n",
       " 0.85001296,\n",
       " 0.62698066,\n",
       " 0.93776166,\n",
       " 3.1334808,\n",
       " 0.10268262,\n",
       " 1.6226909,\n",
       " 0.74477154,\n",
       " 0.012762582,\n",
       " 0.004065079,\n",
       " 1.5088906,\n",
       " 0.7647586,\n",
       " 1.1687075,\n",
       " 0.04022556,\n",
       " 1.0253103,\n",
       " 0.08239329,\n",
       " 1.1555649,\n",
       " 0.51710004,\n",
       " 7.0935203e-06,\n",
       " 0.142881,\n",
       " 2.0400348,\n",
       " 1.4111048,\n",
       " 0.010941708,\n",
       " 8.261284,\n",
       " 0.37712738,\n",
       " 0.0037884398,\n",
       " 0.80357504,\n",
       " 0.910596,\n",
       " 0.3981317,\n",
       " 0.48639107,\n",
       " 3.4600763,\n",
       " 0.002561848,\n",
       " 0.08193505,\n",
       " 0.027780853,\n",
       " 0.03609684,\n",
       " 0.3849662,\n",
       " 0.003713756,\n",
       " 0.33529866,\n",
       " 0.00511491,\n",
       " 7.3130273e-06,\n",
       " 3.0907588e-06,\n",
       " 0.078226194,\n",
       " 1.9410558,\n",
       " 0.184151,\n",
       " 2.836965,\n",
       " 0.3606449,\n",
       " 3.5915604,\n",
       " 2.016461,\n",
       " 0.33557025,\n",
       " 2.2976071e-06,\n",
       " 0.86244327,\n",
       " 2.1141376,\n",
       " 1.9748458,\n",
       " 0.059165224,\n",
       " 2.1807113,\n",
       " 0.16185616,\n",
       " 0.005366142,\n",
       " 0.135318,\n",
       " 0.60535455,\n",
       " 0.09047701,\n",
       " 0.55814564,\n",
       " 3.3977373,\n",
       " 0.19113655,\n",
       " 0.14620322,\n",
       " 0.04745338,\n",
       " 0.0021910234,\n",
       " 1.4605511,\n",
       " 0.16279341,\n",
       " 0.43270358,\n",
       " 1.3840121,\n",
       " 0.0074853944,\n",
       " 0.0056130607,\n",
       " 0.0168409,\n",
       " 2.606117,\n",
       " 0.18184634,\n",
       " 1.330186,\n",
       " 0.34403634,\n",
       " 2.0559502,\n",
       " 0.3730284,\n",
       " 0.7461692,\n",
       " 2.1331904,\n",
       " 2.1333008,\n",
       " 1.7541883,\n",
       " 3.4538307,\n",
       " 1.712398,\n",
       " 0.13662133,\n",
       " 0.23554337,\n",
       " 1.0982056,\n",
       " 0.026593763,\n",
       " 2.046195e-05,\n",
       " 0.73487353,\n",
       " 0.7708848,\n",
       " 0.55659246,\n",
       " 0.56950957,\n",
       " 0.00035194887,\n",
       " 0.006950062,\n",
       " 0.00022380655,\n",
       " 0.47849342,\n",
       " 0.22884285,\n",
       " 0.008037806,\n",
       " 0.50269073,\n",
       " 1.0440291,\n",
       " 2.1758604,\n",
       " 0.14144324,\n",
       " 0.37223756,\n",
       " 0.7895237,\n",
       " 0.7091062,\n",
       " 0.1050944,\n",
       " 0.00042095684,\n",
       " 0.0015318675,\n",
       " 0.3550417,\n",
       " 1.9896994,\n",
       " 0.6267185,\n",
       " 0.8115076,\n",
       " 0.3908104,\n",
       " 1.9859408,\n",
       " 3.1094623,\n",
       " 4.623546,\n",
       " 0.7379746,\n",
       " 0.007576888,\n",
       " 2.4799636e-05,\n",
       " 1.6242782,\n",
       " 2.0020776,\n",
       " 1.0599821,\n",
       " 0.39758447,\n",
       " 0.00023346275,\n",
       " 0.0024349117,\n",
       " 0.00017620699,\n",
       " 0.3432406,\n",
       " 0.8084402,\n",
       " 0.15553972,\n",
       " 0.2737203,\n",
       " 0.3959219,\n",
       " 3.5102222,\n",
       " 0.02453731,\n",
       " 0.6726977,\n",
       " 0.39365864,\n",
       " 0.0782306,\n",
       " 0.00012481428,\n",
       " 0.78720593,\n",
       " 1.1728448,\n",
       " 0.31029037,\n",
       " 0.19673395,\n",
       " 0.0027212289,\n",
       " 0.030299924,\n",
       " 0.0011947046,\n",
       " 0.015168077,\n",
       " 0.66892564,\n",
       " 0.68579924,\n",
       " 9.222497e-05,\n",
       " 0.096702024,\n",
       " 1.7643697,\n",
       " 1.1373804,\n",
       " 0.4246246,\n",
       " 0.00036207025,\n",
       " 0.0002812614,\n",
       " 0.00040979317,\n",
       " 0.5344747,\n",
       " 0.44035286,\n",
       " 0.071100436,\n",
       " 0.13168274,\n",
       " 0.20645712,\n",
       " 0.008814585,\n",
       " 0.87151647,\n",
       " 0.18803872,\n",
       " 0.640445,\n",
       " 0.16962689,\n",
       " 0.00014324076,\n",
       " 0.0022586167,\n",
       " 0.18738788,\n",
       " 1.0314046,\n",
       " 0.45897794,\n",
       " 0.79897845,\n",
       " 4.4311666,\n",
       " 0.054818485,\n",
       " 0.51589614,\n",
       " 0.26880062,\n",
       " 1.0237683e-05,\n",
       " 1.0748214,\n",
       " 1.8551162,\n",
       " 0.65608865,\n",
       " 0.4911987,\n",
       " 0.014518325,\n",
       " 0.00072941376,\n",
       " 0.00073655596,\n",
       " 0.5056962,\n",
       " 1.1055906,\n",
       " 0.8532568,\n",
       " 1.6557236,\n",
       " 0.035816297,\n",
       " 0.0068246466,\n",
       " 0.5425248,\n",
       " 2.244696,\n",
       " 0.73160034,\n",
       " 0.8708844,\n",
       " 1.396341,\n",
       " 0.16165881,\n",
       " 0.14527242,\n",
       " 0.009543076,\n",
       " 0.00046240014,\n",
       " 0.028975947,\n",
       " 0.06874013,\n",
       " 3.4814985,\n",
       " 0.02043079,\n",
       " 1.6644864,\n",
       " 0.8258423,\n",
       " 2.401118,\n",
       " 0.017062856,\n",
       " 0.7879627,\n",
       " 0.19153292,\n",
       " 6.750559e-05,\n",
       " 1.2562579,\n",
       " 0.7643994,\n",
       " 1.0991882,\n",
       " 0.46650037,\n",
       " 0.00073036586,\n",
       " 0.0015825474,\n",
       " 0.00041139877,\n",
       " 0.22979699,\n",
       " 0.28046733,\n",
       " 0.8143151,\n",
       " 4.084016,\n",
       " 0.006357611,\n",
       " 0.18804221,\n",
       " 0.4033664,\n",
       " 0.0016444988,\n",
       " 0.32126492,\n",
       " 0.15155578,\n",
       " 0.0003149861,\n",
       " 6.223683e-05,\n",
       " 0.024295965,\n",
       " 1.6121205,\n",
       " 0.08513916,\n",
       " 0.6961349,\n",
       " 0.0056375577,\n",
       " 0.004784284,\n",
       " 0.8988097,\n",
       " 0.44653663,\n",
       " 6.2422536e-05,\n",
       " 0.027834792,\n",
       " 1.5617138,\n",
       " 1.0012089,\n",
       " 0.73149854,\n",
       " 0.00015232794,\n",
       " 0.004859791,\n",
       " 0.0009042105,\n",
       " 0.34009802,\n",
       " 0.18004414,\n",
       " 0.74522173,\n",
       " 0.16174167,\n",
       " 0.04395789,\n",
       " 0.7217618,\n",
       " 0.052254375,\n",
       " 0.0006019717,\n",
       " 1.1146891,\n",
       " 1.0608196,\n",
       " 0.058995165,\n",
       " 0.0007504968,\n",
       " 0.004245297,\n",
       " 0.23378316,\n",
       " 1.0643773,\n",
       " 0.6313651,\n",
       " 1.4872606,\n",
       " 0.00011733294,\n",
       " 0.0029403702,\n",
       " 0.12915991,\n",
       " 0.648423,\n",
       " 0.58354956,\n",
       " 4.3729295e-05,\n",
       " 0.09274603,\n",
       " 1.1539758,\n",
       " 1.196014,\n",
       " 0.19709381,\n",
       " 0.00011590847,\n",
       " 0.0020513812,\n",
       " 0.000100713434,\n",
       " 0.5580754,\n",
       " 0.48169166,\n",
       " 2.1606734,\n",
       " 1.0232118,\n",
       " 2.2090826,\n",
       " 0.033062838,\n",
       " 0.7741577,\n",
       " 0.98892665,\n",
       " 2.0408015,\n",
       " 0.23075736,\n",
       " 0.09234657,\n",
       " 0.59356827,\n",
       " 0.00046194711,\n",
       " 0.34352317,\n",
       " 4.296258,\n",
       " 0.93895507,\n",
       " 2.7344098,\n",
       " 0.32916796,\n",
       " 0.5446044,\n",
       " 1.8649853,\n",
       " 0.008847427,\n",
       " 0.057075452,\n",
       " 0.7626772,\n",
       " 0.14810993,\n",
       " 1.3285703e-06,\n",
       " 0.06697707,\n",
       " 1.6614113,\n",
       " 1.0137463,\n",
       " 0.14090604,\n",
       " 0.0006408639,\n",
       " 0.00014321701,\n",
       " 0.008289591,\n",
       " 0.0021510357,\n",
       " 0.46057847,\n",
       " 0.66415465,\n",
       " 3.1401353,\n",
       " 0.69715035,\n",
       " 0.46094865,\n",
       " 0.03535758,\n",
       " 0.21944371,\n",
       " 0.811092,\n",
       " 1.1860468,\n",
       " 0.6427788,\n",
       " 0.34067208,\n",
       " 0.00014530352,\n",
       " 0.0400641,\n",
       " 1.1376431,\n",
       " 0.6327663,\n",
       " 0.42133093,\n",
       " 0.045081135,\n",
       " 0.0055894004,\n",
       " 0.002002782,\n",
       " 0.6207493,\n",
       " 0.89062345,\n",
       " 0.02735426,\n",
       " 0.002195432,\n",
       " 1.1194974,\n",
       " 1.5661755,\n",
       " 0.56185347,\n",
       " 0.0820813,\n",
       " 6.6219116e-05,\n",
       " 6.826486e-05,\n",
       " 1.2965123e-05,\n",
       " 7.127157e-05,\n",
       " 0.6560842,\n",
       " 1.8863547,\n",
       " 2.0082178,\n",
       " 2.980493,\n",
       " 0.24450177,\n",
       " 0.030598374,\n",
       " 1.5972056,\n",
       " 0.55104256,\n",
       " 0.036681436,\n",
       " 0.04806167,\n",
       " 0.27261364,\n",
       " 0.31214958,\n",
       " 0.11803945,\n",
       " 0.04102094,\n",
       " 0.38388127,\n",
       " 0.01403906,\n",
       " 1.1980605,\n",
       " 0.26969814,\n",
       " 0.00027735112,\n",
       " 1.3506851,\n",
       " 1.8003099,\n",
       " 1.1112206,\n",
       " 0.049277697,\n",
       " 4.0262094e-05,\n",
       " 1.7225806e-05,\n",
       " 0.0011183033,\n",
       " 5.294346e-05,\n",
       " 0.42265213,\n",
       " 2.0168276,\n",
       " 0.33169493,\n",
       " 2.4125915,\n",
       " 0.41791195,\n",
       " 2.1173358,\n",
       " 0.33460772,\n",
       " 0.9910028,\n",
       " 0.09267248,\n",
       " 2.3099728,\n",
       " 0.12987447,\n",
       " 0.57409936,\n",
       " 0.00021507652,\n",
       " 3.3390617e-05,\n",
       " 0.028805366,\n",
       " 0.5246893,\n",
       " 0.1562103,\n",
       " 0.00034436132,\n",
       " 0.41628495,\n",
       " 0.050504074,\n",
       " 0.00015289795,\n",
       " 0.0024152342,\n",
       " 0.5983028,\n",
       " 0.19283919,\n",
       " 0.00039924105,\n",
       " 1.8005649,\n",
       " 2.2174292,\n",
       " 1.0197731,\n",
       " 0.04765979,\n",
       " 0.0018924656,\n",
       " 8.980204e-05,\n",
       " 0.0006635104,\n",
       " 0.0001005399,\n",
       " 0.32031214,\n",
       " 2.399798,\n",
       " 1.7381897,\n",
       " 2.6025136,\n",
       " 1.3967079,\n",
       " 2.4770465,\n",
       " 0.0023935488,\n",
       " 0.5204616,\n",
       " 0.9842893,\n",
       " 0.00062850997,\n",
       " 0.014910836,\n",
       " 6.303392e-05,\n",
       " 2.2890363,\n",
       " 0.26058337,\n",
       " 0.0010370117,\n",
       " 0.002612833,\n",
       " 5.3716185e-06,\n",
       " 0.014959494,\n",
       " 1.0392206,\n",
       " 0.6470628,\n",
       " 0.0046549793,\n",
       " 1.0393636,\n",
       " 1.5159017,\n",
       " 0.9029367,\n",
       " 0.07227129,\n",
       " 0.07439173,\n",
       " 0.07116404,\n",
       " 0.9394835,\n",
       " 0.007326073,\n",
       " 0.020652093,\n",
       " 0.48945528,\n",
       " 1.6259406,\n",
       " 2.6345382,\n",
       " 2.8452477,\n",
       " 4.8095417,\n",
       " 0.69392866,\n",
       " 0.14135906,\n",
       " 1.9324106,\n",
       " 2.0661802,\n",
       " 0.64871,\n",
       " 0.68511426,\n",
       " 0.46210384,\n",
       " 0.032028653,\n",
       " 1.9091561,\n",
       " 1.043434,\n",
       " 1.7038918,\n",
       " 0.018005634,\n",
       " 0.051904943,\n",
       " 1.7301517,\n",
       " 1.7935294,\n",
       " 4.179734,\n",
       " 0.56565744,\n",
       " 2.237746,\n",
       " 0.39166486,\n",
       " 0.008771121,\n",
       " 0.5062196,\n",
       " 0.011405713,\n",
       " 5.0039136e-05,\n",
       " 0.0032700815,\n",
       " 0.02055874,\n",
       " 0.8278749,\n",
       " 3.3654506,\n",
       " 1.1309096,\n",
       " 0.92699933,\n",
       " 0.14130071,\n",
       " 2.1440153,\n",
       " 0.012515292,\n",
       " 4.1005507,\n",
       " 0.048944432,\n",
       " 0.014500793,\n",
       " 0.0040941755,\n",
       " 0.76260716,\n",
       " 1.6834462,\n",
       " 3.9013972,\n",
       " 0.4014961,\n",
       " 2.8307753,\n",
       " 3.6966858,\n",
       " 1.9761629,\n",
       " 0.0006858214,\n",
       " 4.377789,\n",
       " 1.832316,\n",
       " 4.475767,\n",
       " 0.001133348,\n",
       " 5.9379344,\n",
       " 0.20170611,\n",
       " 0.07399689,\n",
       " 5.262073,\n",
       " 0.030626388,\n",
       " 0.2745857,\n",
       " 1.2067802,\n",
       " 0.01024004,\n",
       " 0.15459096,\n",
       " 3.1663866,\n",
       " 2.7749808,\n",
       " 0.021738375,\n",
       " 0.00038494114,\n",
       " 3.4077861,\n",
       " 3.4533057,\n",
       " 2.649125,\n",
       " 0.036850926,\n",
       " 1.122241,\n",
       " 0.08745098,\n",
       " 5.214114e-05,\n",
       " 1.1963077,\n",
       " 0.03540587,\n",
       " 5.3157797e-05,\n",
       " 0.7240696,\n",
       " 0.19158861,\n",
       " 0.0021893778,\n",
       " 0.00014684918,\n",
       " 0.0029766904,\n",
       " 0.0040620146,\n",
       " 1.8135107,\n",
       " 2.3650975,\n",
       " 0.7427441,\n",
       " 0.04191667,\n",
       " 3.309529e-05,\n",
       " 0.00932183,\n",
       " 0.00904623,\n",
       " 1.8206517,\n",
       " 0.0057546217,\n",
       " 0.0006982827,\n",
       " 3.2482874,\n",
       " 0.22752587,\n",
       " 1.310889,\n",
       " 0.6524604,\n",
       " 2.5251877,\n",
       " 0.3337957,\n",
       " 0.329944,\n",
       " 0.08892909,\n",
       " 3.3032184,\n",
       " 3.7770926e-05,\n",
       " 1.8275232,\n",
       " 0.66493094,\n",
       " 0.648652,\n",
       " 0.003328925,\n",
       " 5.9334798e-05,\n",
       " 0.7179033,\n",
       " 2.2215416,\n",
       " 3.107807,\n",
       " 1.7707014,\n",
       " 2.7527444,\n",
       " 1.9801017,\n",
       " 4.6958394,\n",
       " 0.023259249,\n",
       " 4.1338296,\n",
       " 0.5261047,\n",
       " 0.052053627,\n",
       " 0.008820105,\n",
       " 0.018505257,\n",
       " 1.4027724,\n",
       " 0.020504665,\n",
       " 0.0011606466,\n",
       " 0.4042762,\n",
       " 2.0955496,\n",
       " 2.1954122,\n",
       " 0.17031121,\n",
       " 0.24000227,\n",
       " 0.0015026937,\n",
       " 1.8054667,\n",
       " 0.75656563,\n",
       " 0.017317938,\n",
       " 3.416498,\n",
       " 1.049136,\n",
       " 0.091791496,\n",
       " 0.0014701316,\n",
       " 0.00014155784,\n",
       " 0.0029300768,\n",
       " 0.13684413,\n",
       " 0.5668379,\n",
       " 0.004292828,\n",
       " 0.00027939858,\n",
       " 0.17149012,\n",
       " 1.1920881,\n",
       " 0.03690888,\n",
       " 0.0026047772,\n",
       " 9.9303485e-05,\n",
       " 0.13634352,\n",
       " 0.011074026,\n",
       " 0.01909777,\n",
       " 0.027587522,\n",
       " 0.91112983,\n",
       " 0.056149848,\n",
       " 0.0014271907,\n",
       " 0.00017508668,\n",
       " 0.0020913396,\n",
       " 0.4150748,\n",
       " 1.1443566e-05,\n",
       " 5.32569e-06,\n",
       " 0.04093307,\n",
       " 0.14378077,\n",
       " 1.1330935,\n",
       " 0.09259413,\n",
       " 2.0604396,\n",
       " 2.2572367,\n",
       " 0.0005493355,\n",
       " 4.0544057,\n",
       " 0.049100254,\n",
       " 0.028226295,\n",
       " 0.052974127,\n",
       " 0.8268649,\n",
       " 0.80138934,\n",
       " 0.0011242224,\n",
       " 0.106407516,\n",
       " 0.20305784,\n",
       " 1.5332931,\n",
       " 0.41386834,\n",
       " 0.0002402359,\n",
       " 0.00040325042,\n",
       " 0.037133284,\n",
       " 1.9497608,\n",
       " 0.011478606,\n",
       " 0.13915342,\n",
       " 0.49047026,\n",
       " 0.20401797,\n",
       " 0.018193597,\n",
       " 0.00024383231,\n",
       " 0.00988331,\n",
       " 1.5877628,\n",
       " 0.13227203,\n",
       " 0.0007571232,\n",
       " 0.19871175,\n",
       " 0.055171885,\n",
       " 3.6110956e-05,\n",
       " 0.94925666,\n",
       " 0.100205794,\n",
       " 0.0012496692,\n",
       " 0.0021439642,\n",
       " 0.75853163,\n",
       " 0.13526978,\n",
       " 0.0095318565,\n",
       " 0.7802337,\n",
       " 0.953557,\n",
       " 0.033501834,\n",
       " 0.0059349327,\n",
       " 0.00024115582,\n",
       " 7.26532e-08,\n",
       " 2.9351413e-06,\n",
       " 0.3282844,\n",
       " 0.025874345,\n",
       " 1.6450349e-06,\n",
       " 1.29794025e-05,\n",
       " 9.262942e-05,\n",
       " 0.00023316182,\n",
       " 0.0007814962,\n",
       " 5.194357e-05,\n",
       " 0.02120583,\n",
       " 6.065456e-05,\n",
       " 1.9423892e-06,\n",
       " 0.58340263,\n",
       " 1.1194132e-05,\n",
       " 2.5413776e-06,\n",
       " 0.000296357,\n",
       " 0.00025936938,\n",
       " 0.0033143596,\n",
       " 0.71258074,\n",
       " 5.9318924,\n",
       " 2.8775687,\n",
       " 3.9533343,\n",
       " 0.03792104,\n",
       " 0.14823052,\n",
       " 4.596715,\n",
       " 5.27245,\n",
       " 4.600033,\n",
       " 0.18975466,\n",
       " 0.0021516567,\n",
       " 0.00010906459,\n",
       " 0.003494224,\n",
       " 0.36929816,\n",
       " 5.791187e-07,\n",
       " 1.5119656e-05,\n",
       " 0.42504513,\n",
       " 0.51334065,\n",
       " 0.44345665,\n",
       " 0.00896587,\n",
       " 2.3858927e-05,\n",
       " 1.27833e-05,\n",
       " 0.0058507985,\n",
       " 0.0013658263,\n",
       " 0.0058708126,\n",
       " 1.3612845,\n",
       " 0.33806682,\n",
       " 4.06157,\n",
       " 1.4386721,\n",
       " 0.556272,\n",
       " 0.03980408,\n",
       " 0.00086098944,\n",
       " 0.061433166,\n",
       " 0.016790505,\n",
       " 8.383092e-05,\n",
       " 2.543933e-05,\n",
       " 0.685061,\n",
       " 1.4902632,\n",
       " 4.2460856,\n",
       " 2.2181876,\n",
       " 0.6672062,\n",
       " 0.010355475,\n",
       " 3.850587,\n",
       " 0.0005389183,\n",
       " 0.67065775,\n",
       " 4.540193,\n",
       " 0.03782468,\n",
       " 3.4671102,\n",
       " 0.0054758205,\n",
       " 3.6035466,\n",
       " 0.0052144933,\n",
       " 0.0030604315,\n",
       " 2.4466648,\n",
       " 0.5540863,\n",
       " 0.0019369036,\n",
       " 9.841747e-05,\n",
       " 1.5997134,\n",
       " 0.0011028306,\n",
       " 0.011735807,\n",
       " 2.6233594,\n",
       " 2.4136343,\n",
       " 0.46263245,\n",
       " 0.62618077,\n",
       " 5.9186373,\n",
       " 1.6296383,\n",
       " 0.005401794,\n",
       " 0.00015716806,\n",
       " 3.4688037e-06,\n",
       " 0.0015413233,\n",
       " 0.033972982,\n",
       " 3.3742385e-08,\n",
       " 1.0653754e-05,\n",
       " 0.42493165,\n",
       " 0.060403164,\n",
       " 0.003607479,\n",
       " 0.00023579004,\n",
       " 1.653251e-05,\n",
       " 2.692231e-06,\n",
       " 0.0020427415,\n",
       " 0.0018099485,\n",
       " 0.008121874,\n",
       " 0.11264442,\n",
       " 0.3573359,\n",
       " 0.00052046473,\n",
       " 3.519796e-05,\n",
       " 0.00372289,\n",
       " 2.6916961e-05,\n",
       " 0.0034159506,\n",
       " 3.7462378,\n",
       " 1.5539804,\n",
       " 0.3650912,\n",
       " 0.00040889165,\n",
       " 0.00070781953,\n",
       " 3.8375958e-05,\n",
       " 0.0020592758,\n",
       " 0.015372966,\n",
       " 1.140281,\n",
       " 0.31551546,\n",
       " 4.3945115e-06,\n",
       " 8.699007e-05,\n",
       " 0.6981616,\n",
       " 0.4103346,\n",
       " 0.8641584,\n",
       " 0.033927333,\n",
       " 3.6483168e-06,\n",
       " 5.096904e-06,\n",
       " 0.029719695,\n",
       " 0.0025395965,\n",
       " 0.012560079,\n",
       " 0.037158296,\n",
       " 0.36947227,\n",
       " 2.0484142e-05,\n",
       " 0.3314931,\n",
       " 0.11353143,\n",
       " 0.011578443,\n",
       " 0.0001862824,\n",
       " 1.6830742e-05,\n",
       " 3.8738136e-07,\n",
       " 0.0008666635,\n",
       " 0.008008866,\n",
       " 1.6021652e-08,\n",
       " 4.9127366e-06,\n",
       " 0.024331182,\n",
       " 0.01338229,\n",
       " 0.008326184,\n",
       " 2.408875e-05,\n",
       " 1.9433746e-06,\n",
       " 8.58954e-07,\n",
       " 0.0029981218,\n",
       " 0.0011039086,\n",
       " 0.005247332,\n",
       " 0.045178548,\n",
       " 0.5064576,\n",
       " 0.40038404,\n",
       " 0.017867116,\n",
       " 0.842423,\n",
       " 0.004489012,\n",
       " 0.035100292,\n",
       " 3.1297877e-05,\n",
       " 1.8796974e-05,\n",
       " 0.21652153,\n",
       " 1.7574769,\n",
       " 1.8890762,\n",
       " 0.10920017,\n",
       " 0.00033592383,\n",
       " 1.2120998,\n",
       " 0.0022347057,\n",
       " 3.6703404e-06,\n",
       " 1.9878762,\n",
       " 0.089275904,\n",
       " 0.24989836,\n",
       " 0.0023105096,\n",
       " 0.029796897,\n",
       " 1.8384218,\n",
       " 0.07002173,\n",
       " 1.2886705,\n",
       " 2.840104,\n",
       " 2.9746635,\n",
       " 0.33126673,\n",
       " 0.002134331,\n",
       " 0.7673521,\n",
       " 0.83606505,\n",
       " 1.4027592,\n",
       " 3.2643902,\n",
       " 1.6704707,\n",
       " 0.22792956,\n",
       " 0.75652134,\n",
       " ...]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_entropy_scipy(pre_output.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "57c12049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers_iou_div_mod(pre_output_proba_topk, model, precomp=None):\n",
    "\n",
    "    ious = []\n",
    "\n",
    "    # for each layer\n",
    "    for k, l in enumerate(model.model.layers):\n",
    "\n",
    "        # print(k)\n",
    "        # recover normalized from precomputed (if generated / hooks), or just read from network state\n",
    "        if precomp:\n",
    "            mynorm = precomp[k]\n",
    "        else:\n",
    "            mynorm = l.mynorm\n",
    "\n",
    "        l_ = l.unembed_matrix(mynorm).detach()\n",
    "        layer_topk = get_topn_dict(l_)\n",
    "\n",
    "        # for each token\n",
    "        layer_ious = []\n",
    "        for a, b in zip(pre_output_proba_topk, layer_topk):\n",
    "            a = a[\"top_n_indices\"]\n",
    "            b = b[\"top_n_indices\"]\n",
    "            layer_ious.append(jaccard_similarity(a, b))\n",
    "        ious.append(layer_ious)\n",
    "\n",
    "    return ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6ed1d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update get_topn_dict to use args.topn_threshold\n",
    "def get_topn_dict(logits, threshold=0.9):\n",
    "    # Function remains the same, just using args.topn_threshold as default value\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    sorted_probs, sorted_indices = torch.sort(probabilities, dim=-1, descending=True)\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "    # cutoff\n",
    "    mask = cumulative_probs >= threshold\n",
    "    top_n_lengths = mask.int().argmax(dim=-1) + 1\n",
    "\n",
    "    # gather required only\n",
    "    top_n_values = torch.gather(logits, dim=-1, index=sorted_indices)\n",
    "    top_n_probs = sorted_probs\n",
    "    top_n_indices = sorted_indices\n",
    "\n",
    "    batched_results = []\n",
    "    for i in range(logits.shape[1]):\n",
    "        n = top_n_lengths[0][i].item()\n",
    "        batched_results.append(\n",
    "            {\n",
    "                \"top_n_probs\": top_n_probs[0, i, :n], # .to(torch.float32).cpu().numpy().tolist(),\n",
    "                \"top_n_indices\": top_n_indices[0, i, :n] # .to(torch.float32).cpu().numpy().tolist(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return batched_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "38e0995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers_kl_div_mod(pre_output, model, precomp=None):\n",
    "    softmaxed_log = torch.log_softmax(pre_output, dim=-1)\n",
    "\n",
    "    kls = []\n",
    "    for k, l in enumerate(model.model.layers):\n",
    "        # print(softmaxed_log.shape, l.shape)\n",
    "\n",
    "        # recover normalized from precomputed (if generated / hooks), or just read from network state\n",
    "        if precomp:\n",
    "            mynorm = precomp[k]\n",
    "        else:\n",
    "            mynorm = l.mynorm\n",
    "\n",
    "        l_ = l.unembed_matrix(mynorm)\n",
    "        softmaxed_l = torch.log_softmax(l_, dim=-1)\n",
    "        p = softmaxed_l[0]\n",
    "        q = softmaxed_log[0]\n",
    "        mykl = torch.nn.functional.kl_div(p, q, reduce=False, log_target=True).sum(dim=1)\n",
    "        # print(p.sum(), q.sum(), mykl.shape)\n",
    "        kls.append(mykl.detach().cpu().float().numpy())\n",
    "    return kls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cfee575f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(set(list1) & set(list2))\n",
    "    union = len(set(list1)) + len(set(list2)) - intersection\n",
    "    return float(intersection) / union if union != 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af9ad17",
   "metadata": {},
   "source": [
    "### cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6fc7b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = prompts[742] # 742 is shortest\n",
    "p = prompts[881] #prompts[881] # 840 average joe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3384e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.2 ms, sys: 5.64 ms, total: 21.8 ms\n",
      "Wall time: 22.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "from transformers import StaticCache\n",
    "\n",
    "# Initialize prompt cache\n",
    "# prompt_cache = StaticCache(config=model.config, max_batch_size=1, max_cache_len=2**14, device='mps', dtype=torch.float16)\n",
    "\n",
    "# Generate initial prompt input\n",
    "prompt = generate_prompt(p[\"instruction\"], p[\"question\"], p[\"input\"])\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "prompt_ = prompt[:-1]\n",
    "inputs_ = tokenizer(prompt_, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8eb6be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc7a541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68977dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d24d7695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 201 ms, sys: 614 ms, total: 815 ms\n",
      "Wall time: 2.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# as it were in the script\n",
    "with torch.no_grad():\n",
    "    pre_output = model(**inputs, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd88273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.81 s, sys: 1.15 s, total: 2.95 s\n",
      "Wall time: 5.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# as it were in the script\n",
    "\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "448cbd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.3 s, sys: 6.45 s, total: 10.7 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# No cache\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    use_cache=False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e295f0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c460cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e88b9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 245 ms, sys: 8.67 s, total: 8.92 s\n",
      "Wall time: 18.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt_cache = StaticCache(config=model.config, max_batch_size=1, \n",
    "                           max_cache_len = 2**12 + 64, \n",
    "                           device='mps', dtype=torch.bfloat16)\n",
    "\n",
    "# Generate cache\n",
    "with torch.no_grad():\n",
    "    pre_output = model(**inputs_, past_key_values=prompt_cache, use_cache=False)\n",
    "    prompt_cache = pre_output.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cffa4346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.69 s, sys: 2.79 s, total: 4.48 s\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# With cache\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    past_key_values=copy.deepcopy(prompt_cache),\n",
    "    use_cache=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65c3965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "285c5090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 202 ms, sys: 5.22 s, total: 5.42 s\n",
      "Wall time: 27.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import DynamicCache\n",
    "\n",
    "try: \n",
    "    prompt_cache\n",
    "    del prompt_cache\n",
    "except: \n",
    "    pass\n",
    "    \n",
    "prompt_cache = DynamicCache()\n",
    "\n",
    "# Generate cache\n",
    "with torch.no_grad():\n",
    "    pre_output = model(**inputs_, past_key_values=prompt_cache, use_cache=True)\n",
    "    prompt_cache = pre_output.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92851409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.48 s, sys: 49.7 s, total: 51.2 s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# With cache\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    past_key_values=copy.deepcopy(prompt_cache),\n",
    "    use_cache=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c21b24e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.69 s, sys: 44 s, total: 45.7 s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# resetting cache \n",
    "del prompt_cache\n",
    "prompt_cache = DynamicCache()\n",
    "\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    past_key_values=copy.deepcopy(prompt_cache),\n",
    "    use_cache=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf57362d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# resetting cache \n",
    "del prompt_cache\n",
    "\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    use_cache=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb715701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39953869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "293543d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.37 s, sys: 8.32 s, total: 12.7 s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    past_key_values=copy.deepcopy(prompt_cache),\n",
    "    use_cache=False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8a54ae7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/generation/utils.py:3206\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3203\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3205\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 3206\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3210\u001b[0m     outputs,\n\u001b[1;32m   3211\u001b[0m     model_kwargs,\n\u001b[1;32m   3212\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3213\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    935\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m         position_embeddings,\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:676\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 676\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    689\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:577\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m--> 577\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    581\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:224\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m    222\u001b[0m cos \u001b[38;5;241m=\u001b[39m cos\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m    223\u001b[0m sin \u001b[38;5;241m=\u001b[39m sin\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[0;32m--> 224\u001b[0m q_embed \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (\u001b[43mrotate_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m    225\u001b[0m k_embed \u001b[38;5;241m=\u001b[39m (k \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(k) \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:199\u001b[0m, in \u001b[0;36mrotate_half\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    197\u001b[0m x1 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    198\u001b[0m x2 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m :]\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    use_cache=False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa159870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4956f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af23d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08e74486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.39 s, sys: 5.9 s, total: 9.3 s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Without cache\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    past_key_values=copy.deepcopy(prompt_cache),\n",
    "    use_cache=False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5846f85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.45 s, sys: 1.54 s, total: 2.99 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Reset cache\n",
    "\n",
    "prompt_cache = StaticCache(config=model.config, max_batch_size=1, \n",
    "                           max_cache_len = 2**12, \n",
    "                           device='mps', dtype=torch.bfloat16)\n",
    "\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    past_key_values=copy.deepcopy(prompt_cache),\n",
    "    use_cache=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e81024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39761220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1aa56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01b676a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05123688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.41 s, sys: 7.7 s, total: 9.11 s\n",
      "Wall time: 24.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# With cache\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    past_key_values=copy.deepcopy(prompt_cache),\n",
    "    use_cache=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6ee1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbce1c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5854033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 308 ms, sys: 650 ms, total: 958 ms\n",
      "Wall time: 5.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = generate_prompt(p[\"instruction\"], p[\"question\"], p[\"input\"])\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    pre_output = model(**inputs, use_cache=use_cache)\n",
    "pre_output = pre_output.logits.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a178eeb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1073])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e91459b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16384"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5379a3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ede7d98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 199 ms, sys: 1.16 s, total: 1.36 s\n",
      "Wall time: 9.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import copy\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, StaticCache\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "\n",
    "prompt_cache = StaticCache(config=model.config, max_batch_size=1, max_cache_len=2**14, device='mps', dtype=torch.float16)\n",
    "\n",
    "prompt = generate_prompt(p[\"instruction\"], p[\"question\"], p[\"input\"])\n",
    "prompt_ = prompt[:-1]\n",
    "\n",
    "inputs = tokenizer(prompt_, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pre_output = model(**inputs, past_key_values = prompt_cache)\n",
    "    prompt_cache = pre_output.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c48e6190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 98.4 ms, sys: 367 ms, total: 466 ms\n",
      "Wall time: 3.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_inputs = tokenizer(prompt, return_tensors=\"pt\").to('mps')\n",
    "outputs = model.generate(**new_inputs, past_key_values=prompt_cache, max_new_tokens=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7fcd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540fc108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "089de13b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 81.38 GB, other allocations: 1.84 MB, max allowed: 81.60 GB). Tried to allocate 512.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:5\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/cache_utils.py:1172\u001b[0m, in \u001b[0;36mStaticCache.__init__\u001b[0;34m(self, config, batch_size, max_cache_len, device, dtype, max_batch_size, layer_device_map)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;66;03m# Notes:\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;66;03m# 1. `mark_static_address` is used to tag the cache as an fixed data pointer, preventing cuda graph\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;66;03m#     breaks when updating the cache. It can't be used if the cache code is being compiled (but in that case\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;66;03m#     it is not needed anyway)\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;66;03m# 2. `torch.export()` requires mutations to be registered as buffers.\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m-> 1172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey_cache_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_device\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue_cache_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mzeros(cache_shape, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mlayer_device))\n\u001b[1;32m   1174\u001b[0m     new_layer_key_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey_cache_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 81.38 GB, other allocations: 1.84 MB, max allowed: 81.60 GB). Tried to allocate 512.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf673fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd950a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bf322a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.65 s, sys: 6.02 s, total: 7.67 s\n",
      "Wall time: 29.3 s\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb0e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "132d8a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 13866,   338,  ..., 29937, 13291, 29901]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46db297d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n"
     ]
    }
   ],
   "source": [
    "    num_tokens=20\n",
    "    \n",
    "    outputs = model(**inputs, use_cache=True, return_dict=True)\n",
    "    past = outputs.past_key_values\n",
    "\n",
    "    generated = input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "753035c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 81.61 GB, other allocations: 3.81 MB, max allowed: 81.60 GB). Tried to allocate 8.64 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_tokens):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# For each new step, only pass the last token along with the cache\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     next_input \u001b[38;5;241m=\u001b[39m generated[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m----> 6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m      8\u001b[0m     past \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpast_key_values  \u001b[38;5;66;03m# Update the cache with new key/value pairs\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    935\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m         position_embeddings,\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:676\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 676\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    689\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:602\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[1;32m    600\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m causal_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m q_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    611\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    612\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 81.61 GB, other allocations: 3.81 MB, max allowed: 81.60 GB). Tried to allocate 8.64 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "    for k in range(5):\n",
    "        print(k)\n",
    "        for _ in range(num_tokens):\n",
    "            # For each new step, only pass the last token along with the cache\n",
    "            next_input = generated[:, -1:]\n",
    "            outputs = model(next_input, past_key_values=past, use_cache=True, return_dict=True)\n",
    "            logits = outputs.logits\n",
    "            past = outputs.past_key_values  # Update the cache with new key/value pairs\n",
    "\n",
    "            # Greedily select the next token (alternatively, you can sample)\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            generated = torch.cat((generated, next_token), dim=-1)\n",
    "\n",
    "        # Decode the full sequence of tokens into text\n",
    "        pippo = tokenizer.decode(generated[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6837420a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06964fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbeec7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_key_values = copy.deepcopy(prompt_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc0c6bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'batch_size',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_max_cache_shape',\n",
       " 'get_max_length',\n",
       " 'get_parameter',\n",
       " 'get_seq_length',\n",
       " 'get_submodule',\n",
       " 'get_usable_length',\n",
       " 'half',\n",
       " 'head_dim',\n",
       " 'ipu',\n",
       " 'key_cache',\n",
       " 'key_cache_0',\n",
       " 'key_cache_1',\n",
       " 'key_cache_10',\n",
       " 'key_cache_11',\n",
       " 'key_cache_12',\n",
       " 'key_cache_13',\n",
       " 'key_cache_14',\n",
       " 'key_cache_15',\n",
       " 'key_cache_16',\n",
       " 'key_cache_17',\n",
       " 'key_cache_18',\n",
       " 'key_cache_19',\n",
       " 'key_cache_2',\n",
       " 'key_cache_20',\n",
       " 'key_cache_21',\n",
       " 'key_cache_22',\n",
       " 'key_cache_23',\n",
       " 'key_cache_24',\n",
       " 'key_cache_25',\n",
       " 'key_cache_26',\n",
       " 'key_cache_27',\n",
       " 'key_cache_28',\n",
       " 'key_cache_29',\n",
       " 'key_cache_3',\n",
       " 'key_cache_30',\n",
       " 'key_cache_31',\n",
       " 'key_cache_4',\n",
       " 'key_cache_5',\n",
       " 'key_cache_6',\n",
       " 'key_cache_7',\n",
       " 'key_cache_8',\n",
       " 'key_cache_9',\n",
       " 'load_state_dict',\n",
       " 'max_cache_len',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'num_key_value_heads',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'reorder_cache',\n",
       " 'requires_grad_',\n",
       " 'reset',\n",
       " 'seen_tokens',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'update',\n",
       " 'value_cache',\n",
       " 'value_cache_0',\n",
       " 'value_cache_1',\n",
       " 'value_cache_10',\n",
       " 'value_cache_11',\n",
       " 'value_cache_12',\n",
       " 'value_cache_13',\n",
       " 'value_cache_14',\n",
       " 'value_cache_15',\n",
       " 'value_cache_16',\n",
       " 'value_cache_17',\n",
       " 'value_cache_18',\n",
       " 'value_cache_19',\n",
       " 'value_cache_2',\n",
       " 'value_cache_20',\n",
       " 'value_cache_21',\n",
       " 'value_cache_22',\n",
       " 'value_cache_23',\n",
       " 'value_cache_24',\n",
       " 'value_cache_25',\n",
       " 'value_cache_26',\n",
       " 'value_cache_27',\n",
       " 'value_cache_28',\n",
       " 'value_cache_29',\n",
       " 'value_cache_3',\n",
       " 'value_cache_30',\n",
       " 'value_cache_31',\n",
       " 'value_cache_4',\n",
       " 'value_cache_5',\n",
       " 'value_cache_6',\n",
       " 'value_cache_7',\n",
       " 'value_cache_8',\n",
       " 'value_cache_9',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(past_key_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc4c0cdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for dimension 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# new_inputs = tokenizer(prompt + \" fottiti\", return_tensors=\"pt\").to('mps')\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/generation/utils.py:3199\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3193\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_initial_cache_position(input_ids, model_kwargs)\n\u001b[1;32m   3195\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_unfinished_sequences(\n\u001b[1;32m   3196\u001b[0m     this_peer_finished, synced_gpus, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice, cur_len\u001b[38;5;241m=\u001b[39mcur_len, max_length\u001b[38;5;241m=\u001b[39mmax_length\n\u001b[1;32m   3197\u001b[0m ):\n\u001b[1;32m   3198\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[0;32m-> 3199\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_inputs_for_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3201\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n\u001b[1;32m   3202\u001b[0m     model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/generation/utils.py:384\u001b[0m, in \u001b[0;36mGenerationMixin.prepare_inputs_for_generation\u001b[0;34m(self, input_ids, past_key_values, attention_mask, inputs_embeds, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m past_key_values\n\u001b[0;32m--> 384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mcache_position\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:  \u001b[38;5;66;03m# Exception 1 or Exception 3\u001b[39;00m\n\u001b[1;32m    385\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m input_ids[:, \u001b[38;5;241m-\u001b[39mcache_position\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] :]\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m cache_position\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:  \u001b[38;5;66;03m# Default case (the \"else\", a no op, is Exception 2)\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index -1 is out of bounds for dimension 0 with size 0"
     ]
    }
   ],
   "source": [
    "# new_inputs = tokenizer(prompt + \" fottiti\", return_tensors=\"pt\").to('mps')\n",
    "outputs = model.generate(**inputs, past_key_values=past_key_values, max_new_tokens=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5981ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def generate_text_with_cache(model, tokenizer, prompt, num_tokens=20):\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model(input_ids, use_cache=True, return_dict=True)\n",
    "    past = outputs.past_key_values  # Cache of previous key/value pairs\n",
    "\n",
    "    generated = input_ids\n",
    "\n",
    "    # Generate tokens one by one using the cached past\n",
    "    for _ in range(num_tokens):\n",
    "        # For each new step, only pass the last token along with the cache\n",
    "        next_input = generated[:, -1:]\n",
    "        outputs = model(next_input, past_key_values=past, use_cache=True, return_dict=True)\n",
    "        logits = outputs.logits\n",
    "        past = outputs.past_key_values  # Update the cache with new key/value pairs\n",
    "\n",
    "        # Greedily select the next token (alternatively, you can sample)\n",
    "        next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        generated = torch.cat((generated, next_token), dim=-1)\n",
    "\n",
    "    # Decode the full sequence of tokens into text\n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the model you want to use; for example, GPT-2\n",
    "    model_name = \"gpt2\"  # Change this to your desired model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    prompt = \"Hello, how are you?\"\n",
    "    print(\"Prompt:\", prompt)\n",
    "    \n",
    "    # Generate text by extending the prompt with additional tokens\n",
    "    output_text = generate_text_with_cache(model, tokenizer, prompt, num_tokens=20)\n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8258091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c4a0cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e75ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, StaticCache\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "\n",
    "DEVICE, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Init StaticCache with big enough max-length (1024 tokens for the below example)\n",
    "# You can also init a DynamicCache, if that suits you better\n",
    "prompt_cache = StaticCache(config=model.config, max_batch_size=1, max_cache_len=1024, device=DEVICE, dtype=torch.bfloat16)\n",
    "\n",
    "INITIAL_PROMPT = \"You are a helpful assistant. \"\n",
    "inputs_initial_prompt = tokenizer(INITIAL_PROMPT, return_tensors=\"pt\").to(DEVICE)\n",
    "# This is the common prompt cached, we need to run forward without grad to be abel to copy\n",
    "with torch.no_grad():\n",
    "     prompt_cache = model(**inputs_initial_prompt, past_key_values = prompt_cache).past_key_values\n",
    "\n",
    "prompts = [\"Help me to write a blogpost about travelling.\", \"What is the capital of France?\"]\n",
    "responses = []\n",
    "for prompt in prompts:\n",
    "    new_inputs = tokenizer(INITIAL_PROMPT + prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    past_key_values = copy.deepcopy(prompt_cache)\n",
    "    outputs = model.generate(**new_inputs, past_key_values=past_key_values,max_new_tokens=20)\n",
    "    response = tokenizer.batch_decode(outputs)[0]\n",
    "    responses.append(response)\n",
    "\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb64b717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.87 s, sys: 945 ms, total: 2.82 s\n",
      "Wall time: 5.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    use_cache=use_cache\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "539779e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_scores = model.compute_transition_scores(post_output.sequences, post_output.scores, normalize_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "772738ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1094]), 21, torch.Size([1, 32001]), torch.Size([1, 21]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_output.sequences.shape, len(post_output.scores), post_output.scores[0].shape, transition_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72a9695f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32001])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_output.scores[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0d2817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7f687ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihoods = [score.item() for score in transition_scores[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5af0080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3557f69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_scores_s = model.compute_transition_scores(post_output.sequences, post_output.scores, normalize_logits=True)\n",
    "log_likelihoods_s = [score.item() for score in transition_scores[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e079b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f264bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-4.768370445162873e-07,\n",
       " -0.09085541218519211,\n",
       " -0.10185058414936066,\n",
       " -4.768370445162873e-07,\n",
       " -1.1920928244535389e-07,\n",
       " -2.407998726994265e-05,\n",
       " -0.0023836076725274324,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.022649873048067093,\n",
       " -0.007525428663939238,\n",
       " -9.536738616588991e-07,\n",
       " -3.576278118089249e-07,\n",
       " 0.0,\n",
       " -2.3841855067985307e-07,\n",
       " 0.0,\n",
       " -3.2186455882765586e-06,\n",
       " -7.867782187531702e-06,\n",
       " -5.960462772236497e-07,\n",
       " -1.1920928244535389e-07,\n",
       " 0.0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_scores_l = model.compute_transition_scores(post_output.sequences, post_output.logits, normalize_logits=True)\n",
    "log_likelihoods_l = [score.item() for score in transition_scores[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91fad576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.7684e-07, -9.0855e-02, -1.0185e-01, -4.7684e-07, -1.1921e-07,\n",
       "         -2.4080e-05, -2.3836e-03,  0.0000e+00,  0.0000e+00, -2.2650e-02,\n",
       "         -7.5254e-03, -9.5367e-07, -3.5763e-07,  0.0000e+00, -2.3842e-07,\n",
       "          0.0000e+00, -3.2186e-06, -7.8678e-06, -5.9605e-07, -1.1921e-07,\n",
       "          0.0000e+00]], device='mps:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ef2d91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01dc728c76464e41bcf26424c58834a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "\n",
    "import scipy.stats\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache\n",
    "\n",
    "\n",
    "# run params\n",
    "temperature=1.0\n",
    "top_p=0.9\n",
    "max_new_tokens=64\n",
    "use_cache=True\n",
    "device = \"mps\"\n",
    "torch_dtype = torch.bfloat16\n",
    "\n",
    "model_name = \"osunlp/TableLlama\"\n",
    "\n",
    "# load inputs\n",
    "input_file = \"turl_test_2k_prompts_50.jsonl\"\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device('mps')\n",
    "\n",
    "# Load input data\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    prompts = [json.loads(line) for line in f]\n",
    "\n",
    "# Load model\n",
    "config = transformers.AutoConfig.from_pretrained(model_name)\n",
    "if model_name.startswith(\"osunlp\"):\n",
    "    orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch_dtype).to(\n",
    "        device\n",
    "    )\n",
    "    model.resize_token_embeddings(32001)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, model_max_length=orig_ctx_len, padding_side=\"left\", use_fast=False\n",
    "    )\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch_dtype).to(\n",
    "        device\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "model.eval()\n",
    "\n",
    "# build prompts\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_seg}\\n\\n### Question:\\n{question}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def generate_prompt(instruction, question, input_seg=None):\n",
    "    question += \" Answer with just a candidate, selected from the provided referent entity candidates list, and nothing else. The selected candidate must be reported verbatim from the list provided as input. Each candidate in the list is enclosed between < and > and reports [DESC] and [TYPE] information.\"\n",
    "    if input_seg:\n",
    "        return PROMPT_DICT[\"prompt_input\"].format(\n",
    "            instruction=instruction, input_seg=input_seg, question=question\n",
    "        )\n",
    "    else:\n",
    "        return PROMPT_DICT[\"prompt_no_input\"].format(instruction=instruction)\n",
    "\n",
    "\n",
    "def flip():\n",
    "    gc.collect()\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    return\n",
    "\n",
    "\n",
    "# Update get_topk_dict to use args.topk\n",
    "def get_topk_dict(logits, k=10):\n",
    "    # Function remains the same, just using args.topk as default value\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    top_k_values, top_k_indices = torch.topk(logits, k=k, dim=-1)\n",
    "    top_k_probs = torch.gather(probabilities, dim=-1, index=top_k_indices)\n",
    "    return {\"top_k_probs\": top_k_probs, \"top_k_indices\": top_k_indices}\n",
    "\n",
    "\n",
    "# Update get_topn_dict to use args.topn_threshold\n",
    "def get_topn_dict(logits, threshold=0.9):\n",
    "    # Function remains the same, just using args.topn_threshold as default value\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    sorted_probs, sorted_indices = torch.sort(probabilities, dim=-1, descending=True)\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "    # cutoff\n",
    "    mask = cumulative_probs >= threshold\n",
    "    top_n_lengths = mask.int().argmax(dim=-1) + 1\n",
    "\n",
    "    # gather required only\n",
    "    top_n_values = torch.gather(logits, dim=-1, index=sorted_indices)\n",
    "    top_n_probs = sorted_probs\n",
    "    top_n_indices = sorted_indices\n",
    "\n",
    "    batched_results = []\n",
    "    for i in range(logits.shape[1]):\n",
    "        n = top_n_lengths[0][i].item()\n",
    "        batched_results.append(\n",
    "            {\n",
    "                \"top_n_probs\": top_n_probs[0, i, :n].to(torch.float32), #.cpu().numpy().tolist(),\n",
    "                \"top_n_indices\": top_n_indices[0, i, :n].to(torch.float32)#.cpu().numpy().tolist(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return batched_results\n",
    "\n",
    "\n",
    "def compute_entropy_scipy(logits):\n",
    "    probabilities = torch.softmax(logits, dim=-1).detach().cpu().float().numpy()\n",
    "    entropy_values = [scipy.stats.entropy(row).tolist() for row in probabilities[0]]\n",
    "    return entropy_values\n",
    "\n",
    "# processes data on device, returns ram object\n",
    "def get_layers_kl_div_mod(pre_output, model, precomp=None):\n",
    "    softmaxed_log = torch.log_softmax(pre_output, dim=-1)\n",
    "\n",
    "    kls = []\n",
    "    for k, l in enumerate(model.model.layers):\n",
    "        # print(softmaxed_log.shape, l.shape)\n",
    "\n",
    "        # recover normalized from precomputed (if generated / hooks), or just read from network state\n",
    "        if precomp:\n",
    "            mynorm = precomp[k]\n",
    "        else:\n",
    "            mynorm = l.mynorm\n",
    "\n",
    "        l_ = l.unembed_matrix(mynorm)\n",
    "        softmaxed_l = torch.log_softmax(l_, dim=-1) # .cpu()\n",
    "        p = softmaxed_l[0]\n",
    "        q = softmaxed_log[0]\n",
    "        mykl = torch.nn.functional.kl_div(p, q, reduce=False, log_target=True).sum(dim=1)\n",
    "        # print(p.sum(), q.sum(), mykl.shape)\n",
    "        kls.append(mykl.detach().cpu().float().numpy().tolist())\n",
    "    return kls\n",
    "\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(set(list1) & set(list2))\n",
    "    union = len(set(list1)) + len(set(list2)) - intersection\n",
    "    return float(intersection) / union if union != 0 else 0.0\n",
    "\n",
    "\n",
    "def get_layers_iou_div_mod(pre_output_proba_topk, model, precomp=None):\n",
    "\n",
    "    ious = []\n",
    "\n",
    "    # for each layer\n",
    "    for k, l in enumerate(model.model.layers):\n",
    "\n",
    "        # print(k)\n",
    "        # recover normalized from precomputed (if generated / hooks), or just read from network state\n",
    "        if precomp:\n",
    "            mynorm = precomp[k]\n",
    "        else:\n",
    "            mynorm = l.mynorm\n",
    "\n",
    "        l_ = l.unembed_matrix(mynorm).detach()\n",
    "        layer_topk = get_topn_dict(l_)\n",
    "\n",
    "        # for each token\n",
    "        layer_ious = []\n",
    "        for a, b in zip(pre_output_proba_topk, layer_topk):\n",
    "            a = a[\"top_n_indices\"]\n",
    "            b = b[\"top_n_indices\"]\n",
    "            layer_ious.append(jaccard_similarity(a, b))\n",
    "        ious.append(layer_ious)\n",
    "\n",
    "    return ious\n",
    "\n",
    "\n",
    "class BlockOutputWrapper(torch.nn.Module):\n",
    "    def __init__(self, block, unembed_matrix, norm):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.unembed_matrix = unembed_matrix\n",
    "        self.norm = norm\n",
    "        self.mynorm = None\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        output = self.block(*args, **kwargs)\n",
    "        self.mynorm = self.norm(output[0])\n",
    "        # self.block_output_unembedded = self.unembed_matrix(self.norm(output[0]))\n",
    "        return output\n",
    "\n",
    "def hook_fn(layer_idx, gen, in_generate=False):\n",
    "    def hook(module, input, output):\n",
    "        if isinstance(output, tuple) and hasattr(module, \"mynorm\"):\n",
    "            clo = module.mynorm.clone().detach()\n",
    "            if in_generate and clo.shape[1] > 1:\n",
    "                clo = clo[\n",
    "                    :, -1:, :\n",
    "                ]  # first token returns all prompt tokens, patch last\n",
    "            gen[layer_idx].append(clo)\n",
    "\n",
    "    return hook\n",
    "\n",
    "for i, layer in enumerate(model.model.layers):\n",
    "    model.model.layers[i] = BlockOutputWrapper(layer, model.lm_head, model.model.norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "910ac8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = type('', (), {})()\n",
    "args.compute_pre_kl = True\n",
    "args.compute_pre_iou = False\n",
    "args.max_new_tokens = 64\n",
    "args.temperature = 1.0\n",
    "args.top_p = 0.9\n",
    "\n",
    "import traceback\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34e13cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.compute_pre_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4adc2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After forward  3.6957175420000112\n",
      "After compute 1  5.061083958999973\n",
      "After compute 2  5.061179833999972\n",
      "After compute 3  5.061185583999986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/1 [02:35<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 11s, sys: 15.7 s, total: 2min 26s\n",
      "Wall time: 2min 35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "NREP = 10\n",
    "MAXTOK = 4096\n",
    "\n",
    "outlist = []\n",
    "with torch.no_grad():\n",
    "    for pid, p in enumerate(tqdm([prompts[840]])):\n",
    "        try:\n",
    "            start = time.perf_counter()\n",
    "            \n",
    "            prompt = generate_prompt(p[\"instruction\"], p[\"question\"], p[\"input\"])\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            # dirty trick for caching\n",
    "            prompt_ = prompt[:-1]\n",
    "            inputs_ = tokenizer(prompt_, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            # ( ! )\n",
    "            if inputs.input_ids.shape[1] > MAXTOK:\n",
    "                continue  # roughly 75% of data kept\n",
    "\n",
    "            prompt_cache = DynamicCache()\n",
    "\n",
    "            # attach hooks if needed\n",
    "            if args.compute_pre_kl or args.compute_pre_iou:\n",
    "                generated_block_pre = {i: [] for i in range(len(model.model.layers))}\n",
    "                hooks = []\n",
    "                for i, layer in enumerate(model.model.layers):\n",
    "                    hook = layer.register_forward_hook(hook_fn(i, generated_block_pre, in_generate=False))\n",
    "                    hooks.append(hook)\n",
    "\n",
    "            pre_output = model(**inputs_, past_key_values=prompt_cache, use_cache=True)\n",
    "            prompt_cache = pre_output.past_key_values\n",
    "            \n",
    "            print(\"After forward \", time.perf_counter() - start)\n",
    "\n",
    "            # detach hooks if needed\n",
    "            if args.compute_pre_kl or args.compute_pre_iou:\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "\n",
    "                # accumulate norms at each generated token\n",
    "                for i in generated_block_pre:\n",
    "                    generated_block_pre[i] = (\n",
    "                        torch.stack(generated_block_pre[i]).squeeze(1).permute(1, 0, 2)\n",
    "                    )\n",
    "                generated_block_pre = list(generated_block_pre.values())\n",
    "\n",
    "            # top-n + top-k\n",
    "            # TODO convert to plain python structures\n",
    "            p[\"pre_output_proba_topn\"] = get_topn_dict(pre_output.logits)\n",
    "            p[\"pre_output_proba_topk\"] = get_topk_dict(pre_output.logits)\n",
    "            p[\"pre_output_true_entropies\"] = compute_entropy_scipy(pre_output.logits)\n",
    "            \n",
    "            print(\"After compute 1 \", time.perf_counter() - start)\n",
    "            \n",
    "            # Add conditional execution of KL/IOU calculations based on args (compute before detaching)\n",
    "            if args.compute_pre_kl:\n",
    "                p[\"pre_output_layers_kl\"] = get_layers_kl_div_mod(pre_output.logits, model, generated_block_pre)\n",
    "            \n",
    "            print(\"After compute 2 \", time.perf_counter() - start)\n",
    "            \n",
    "            if args.compute_pre_iou:\n",
    "                p[\"pre_output_layers_iou\"] = get_layers_iou_div_mod(p[\"pre_output_proba_topn\"], model, generated_block_pre)\n",
    "                \n",
    "            print(\"After compute 3 \", time.perf_counter() - start)\n",
    "            \n",
    "            # to python lists\n",
    "            for ty in p[\"pre_output_proba_topn\"]:\n",
    "                ty['top_n_probs'] = ty['top_n_probs'].detach().cpu().numpy().tolist()\n",
    "                ty['top_n_indices'] = ty['top_n_indices'].detach().cpu().numpy().tolist()\n",
    "            p['pre_output_proba_topk']['top_k_probs'] = p['pre_output_proba_topk']['top_k_probs'].detach().cpu().float().numpy().tolist()\n",
    "            p['pre_output_proba_topk']['top_k_indices'] = p['pre_output_proba_topk']['top_k_indices'].detach().cpu().float().numpy().tolist()\n",
    "\n",
    "            # cleanup\n",
    "            del pre_output\n",
    "            flip()\n",
    "            \n",
    "            p[\"post_output_sequences\"] = []\n",
    "            p[\"post_output_proba_topn\"] = []\n",
    "            p[\"post_output_proba_topk\"] = []\n",
    "            p[\"post_output_true_entropies\"] = []\n",
    "            p[\"post_output_layers_kl\"] = []\n",
    "            p[\"post_output_layers_iou\"] = []\n",
    "            p[\"transition_scores_s\"] = []\n",
    "            p[\"transition_scores_l\"] = []\n",
    "\n",
    "            # NREP generate steps for each prompt\n",
    "            for kk in range(NREP):\n",
    "\n",
    "                generated_block_outputs = {i: [] for i in range(len(model.model.layers))}\n",
    "\n",
    "                # register hooks\n",
    "                hooks = []\n",
    "\n",
    "                for i, layer in enumerate(model.model.layers):\n",
    "                    hook = layer.register_forward_hook(hook_fn(i, generated_block_outputs, in_generate=True))\n",
    "                    hooks.append(hook)\n",
    "\n",
    "                post_output = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=args.max_new_tokens,\n",
    "                    temperature=args.temperature,\n",
    "                    top_p=args.top_p,\n",
    "                    output_scores=True,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_logits=True,\n",
    "                    past_key_values=copy.deepcopy(prompt_cache),\n",
    "                    cache_implementation=None,\n",
    "                    use_cache=True,\n",
    "                    do_sample=True,\n",
    "                )\n",
    "\n",
    "                # remove hooks\n",
    "                for hook in hooks:\n",
    "                    hook.remove()\n",
    "\n",
    "                # accumulate norms at each generated token\n",
    "                for i in generated_block_outputs:\n",
    "                    generated_block_outputs[i] = (\n",
    "                        torch.stack(generated_block_outputs[i]).squeeze(1).permute(1, 0, 2)\n",
    "                    )\n",
    "                generated_block_outputs = list(generated_block_outputs.values())\n",
    "\n",
    "                # sequence\n",
    "                post_output_sequences = post_output.sequences.detach().cpu().numpy().tolist()\n",
    "                p[\"post_output_sequences\"].append(post_output_sequences)\n",
    "\n",
    "                post_output_scores = [pp for pp in post_output.logits]\n",
    "                post_output_scores = torch.stack(post_output_scores, dim=1)\n",
    "\n",
    "                # top-n + top-k\n",
    "                mypost_topn = get_topn_dict(post_output_scores)\n",
    "                mypost_topk = get_topk_dict(post_output_scores)\n",
    "\n",
    "                p[\"post_output_true_entropies\"].append(compute_entropy_scipy(post_output_scores))\n",
    "\n",
    "                # logit lens KL/IOU\n",
    "                p[\"post_output_layers_kl\"].append(\n",
    "                    get_layers_kl_div_mod(post_output_scores, model, generated_block_outputs)\n",
    "                )\n",
    "                p[\"post_output_layers_iou\"].append(\n",
    "                    get_layers_iou_div_mod(mypost_topn, model, generated_block_outputs)\n",
    "                )\n",
    "                \n",
    "                # to python lists\n",
    "                for ty in mypost_topn:\n",
    "                    ty['top_n_probs'] = ty['top_n_probs'].detach().cpu().numpy().tolist()\n",
    "                    ty['top_n_indices'] = ty['top_n_indices'].detach().cpu().numpy().tolist()\n",
    "                mypost_topk['top_k_probs'] = mypost_topk['top_k_probs'].detach().cpu().float().numpy().tolist()\n",
    "                mypost_topk['top_k_indices'] = mypost_topk['top_k_indices'].detach().cpu().float().numpy().tolist()\n",
    "                \n",
    "                p[\"post_output_proba_topn\"].append(mypost_topn)\n",
    "                p[\"post_output_proba_topk\"].append(mypost_topk)\n",
    "\n",
    "                # transition scores\n",
    "                # https://github.com/jlko/semantic_uncertainty/blob/a8d9aa8cecd5f3bec09b19ae38ab13552e0846f4/semantic_uncertainty/uncertainty/models/huggingface_models.py\n",
    "                transition_scores_s = model.compute_transition_scores(\n",
    "                    post_output.sequences, post_output.scores, normalize_logits=True\n",
    "                )\n",
    "                log_likelihoods_s = [score.item() for score in transition_scores_s[0]]\n",
    "                p[\"transition_scores_s\"].append(log_likelihoods_s)\n",
    "                transition_scores_l = model.compute_transition_scores(\n",
    "                    post_output.sequences, post_output.logits, normalize_logits=True\n",
    "                )\n",
    "                log_likelihoods_l = [score.item() for score in transition_scores_l[0]]\n",
    "                p[\"transition_scores_l\"].append(log_likelihoods_l)\n",
    "\n",
    "                # cleanup\n",
    "                del post_output\n",
    "          \n",
    "            flip()    \n",
    "            \n",
    "            # total processing time   \n",
    "            end = time.perf_counter()\n",
    "            p[\"elapsed_pre\"] = end - start\n",
    "            p[\"pid\"] = pid\n",
    "            \n",
    "            outlist.append(p)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"EXCEPTION!\")\n",
    "            import traceback\n",
    "\n",
    "            print(traceback.format_exc())\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141c5edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7054da24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['table', 'cell', 'instruction', 'input', 'question', 'output', 'pre_output_proba_topn', 'pre_output_proba_topk', 'pre_output_true_entropies', 'pre_output_layers_kl', 'post_output_sequences', 'post_output_proba_topn', 'post_output_proba_topk', 'post_output_true_entropies', 'post_output_layers_kl', 'post_output_layers_iou', 'transition_scores_s', 'transition_scores_l', 'elapsed_pre'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6949433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(p['pre_output_proba_topn'][0]['top_n_probs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e67213e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(p['post_output_proba_topn'][0][0]['top_n_probs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba159e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(p['pre_output_proba_topn'][0]['top_n_indices'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd367b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(p['post_output_proba_topn'][0][0]['top_n_indices'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f38c851b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(p['pre_output_true_entropies'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cafccb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(p['pre_output_layers_kl'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1510009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(p['post_output_layers_kl'][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d65cfa33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(p['post_output_true_entropies'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28cf6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b051ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# senza hooks\n",
    "After forward  4.136636542000019\n",
    "After compute 1  5.565906584000004\n",
    "After compute 2  5.566010249999977\n",
    "After compute 3  5.566016542\n",
    "\n",
    "# con hooks\n",
    "After forward  3.8336201669999923\n",
    "After compute 1  5.19361487499998\n",
    "After compute 2  8.089015749999987\n",
    "After compute 3  8.08910745899999\n",
    "\n",
    "# kl e iou\n",
    "After forward  3.7006984999999872\n",
    "After compute 1  5.062452082999982\n",
    "After compute 2  7.588468167000002\n",
    "After compute 3  15.70826975\n",
    "\n",
    "# 10 runs senza kl\n",
    "# 2min 35s\n",
    "\n",
    "# 10 runs con kl\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doubt",
   "language": "python",
   "name": "doubt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
