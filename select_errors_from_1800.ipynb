{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ab59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e332ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"turl_test_2k_prompts_50.jsonl\"\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "model_name = \"osunlp/TableLlama\"\n",
    "config = transformers.AutoConfig.from_pretrained(model_name)\n",
    "orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=orig_ctx_len, padding_side=\"left\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d387cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt formatting\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_seg}\\n\\n### Question:\\n{question}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def generate_prompt(instruction, question, input_seg=None):\n",
    "    question += \" Answer with just a candidate, selected from the provided referent entity candidates list, and nothing else. The selected candidate must be reported verbatim from the list provided as input. Each candidate in the list is enclosed between < and > and reports [DESC] and [TYPE] information.\"\n",
    "    if input_seg:\n",
    "        return PROMPT_DICT[\"prompt_input\"].format(instruction=instruction, input_seg=input_seg, question=question)\n",
    "    else:\n",
    "        return PROMPT_DICT[\"prompt_no_input\"].format(instruction=instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "947af3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 37203968\r\n",
      "drwx------  1 bono  staff   1.0M Feb 17 08:52 \u001b[34m..\u001b[m\u001b[m\r\n",
      "-rwx------@ 1 bono  staff   4.4G Mar  7 12:23 \u001b[31mTableLlama.1548780410.pickle\u001b[m\u001b[m\r\n",
      "drwx------  1 bono  staff   1.0M Mar  7 14:45 \u001b[34m.\u001b[m\u001b[m\r\n",
      "-rwx------  1 bono  staff   4.0K Mar  7 14:46 \u001b[31m._TableLlama.1548780410.pickle\u001b[m\u001b[m\r\n",
      "-rwx------@ 1 bono  staff   4.4G Mar  8 14:55 \u001b[31mTableLlama.2573653229.pickle\u001b[m\u001b[m\r\n",
      "-rwx------  1 bono  staff   4.0K Mar  8 14:56 \u001b[31m._TableLlama.2573653229.pickle\u001b[m\u001b[m\r\n",
      "-rwx------@ 1 bono  staff   4.4G Mar  9 10:21 \u001b[31mTableLlama.1765523202.pickle\u001b[m\u001b[m\r\n",
      "-rwx------  1 bono  staff   4.0K Mar  9 10:27 \u001b[31m._TableLlama.1765523202.pickle\u001b[m\u001b[m\r\n",
      "-rwx------@ 1 bono  staff   4.4G Mar 10 08:18 \u001b[31mTableLlama.940523022.pickle\u001b[m\u001b[m\r\n",
      "-rwx------  1 bono  staff   4.0K Mar 10 08:35 \u001b[31m._TableLlama.940523022.pickle\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls -larth \"/Volumes/ssd/uncertainty/tablellama su 1800 prompts/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9d97183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TableLlama.1548780410.pickle\n",
      "TableLlama.1765523202.pickle\n",
      "TableLlama.2573653229.pickle\n",
      "TableLlama.940523022.pickle\n",
      "CPU times: user 1min 58s, sys: 49.7 s, total: 2min 48s\n",
      "Wall time: 2min 59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7204"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load processed data\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "prefix = '/Volumes/ssd/uncertainty/tablellama su 1800 prompts/'\n",
    "\n",
    "run = 0\n",
    "\n",
    "outlist = []\n",
    "for file in os.listdir(prefix):\n",
    "    if file.endswith('pickle') and not file.startswith('.'):\n",
    "        print(file)\n",
    "        with open(os.path.join(prefix, file), 'rb') as handle:\n",
    "            outlist_ = pickle.load(handle)\n",
    "           \n",
    "            for pid, o in enumerate(outlist_):\n",
    "                del o['pre_output_proba_topn']\n",
    "                del o['pre_output_proba_topk']\n",
    "                del o['pre_output_true_entropies']\n",
    "                del o['post_output_proba_topn']\n",
    "                del o['post_output_proba_topk']\n",
    "                del o['post_output_true_entropies']\n",
    "                o['run'] = run\n",
    "                o['pid'] = pid\n",
    "                outlist.append(o)\n",
    "        run += 1\n",
    "\n",
    "len(outlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6ce8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if False:\n",
    "    with open('1800.pickle', 'wb') as handle:\n",
    "        pickle.dump(outlist, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open('1800.pickle', 'rb') as handle:\n",
    "        outlist = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab98ed2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7204"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f007b83e",
   "metadata": {},
   "source": [
    "### check outputs against ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b59b037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4857 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 2.28 s, total: 1min 11s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# show results\n",
    "\n",
    "c=0\n",
    "t=0\n",
    "h=0\n",
    "\n",
    "truth = []\n",
    "\n",
    "for idx, p in enumerate(outlist):\n",
    "    print(idx, end='\\r')\n",
    "    c+=1\n",
    "    # in\n",
    "    prompt = generate_prompt(p[\"instruction\"], p[\"question\"], p[\"input\"])\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    # computed\n",
    "    # post_output_scores = p['post_output_scores']\n",
    "    post_output_sequences = p['post_output_sequences']\n",
    "    # bases\n",
    "    baseid = len(inputs[\"input_ids\"][0]) + 1\n",
    "    endid = len(post_output_sequences[0])\n",
    "    # lookout\n",
    "    generated_ids = post_output_sequences\n",
    "    generated_text = tokenizer.decode(generated_ids[0][baseid:endid], skip_special_tokens=True)\n",
    "    # print\n",
    "    # print(generated_text)\n",
    "    # print(p['output'])\n",
    "    # print('\\n')\n",
    "    #test\n",
    "    a = generated_text.lower().strip()\n",
    "    b = p['output'].lower().strip()\n",
    "    # correct\n",
    "    correct = False\n",
    "    if (a in b) or (b in a) or (b.startswith(a)) or (a.startswith(b)): \n",
    "        correct = True\n",
    "        t+=1\n",
    "    # hallucinated\n",
    "    elif a not in prompt.lower().strip(): \n",
    "        h+=1\n",
    "    # incorrect\n",
    "    else:\n",
    "        pass\n",
    "        #print(generated_text)\n",
    "        #print(p['output'])\n",
    "        #print('\\n')\n",
    "        # print(prompt)\n",
    "        # print('\\n')\n",
    "        \n",
    "    truth.append((p['run'], p['pid'], correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc34a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = pd.DataFrame(truth, columns=['run', 'pid', 'correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9840ce60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    1801\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth.groupby('pid').size().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "583aa919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='correct', ylabel='Count'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.histplot(truth.groupby('pid').correct.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52c9aeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accu = truth.groupby('pid').correct.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0efd3d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(344,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu[accu<1.].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62c5662a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "correct\n",
       "0.00    125\n",
       "0.75    107\n",
       "0.50     78\n",
       "0.25     34\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu[accu<1.].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "137c9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all prompts with errors plus a sample of noerrors\n",
    "err_idx = accu[accu<1.].index.tolist()\n",
    "noerr_idx = accu[accu==1.].sample(len(accu[accu<1.]), random_state=42).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f19ee6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(err_idx) & set(noerr_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80143637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "688"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_pids = set(err_idx) | set(noerr_idx)\n",
    "len(selected_pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1f3417c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TableLlama.1548780410.pickle\n",
      "TableLlama.1765523202.pickle\n",
      "TableLlama.2573653229.pickle\n",
      "TableLlama.940523022.pickle\n",
      "CPU times: user 2min 20s, sys: 2min 3s, total: 4min 23s\n",
      "Wall time: 5min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2752"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load processed data\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "prefix = '/Volumes/ssd/uncertainty/tablellama su 1800 prompts/'\n",
    "\n",
    "run = 0\n",
    "\n",
    "outlist = []\n",
    "for file in os.listdir(prefix):\n",
    "    if file.endswith('pickle') and not file.startswith('.'):\n",
    "        print(file)\n",
    "        with open(os.path.join(prefix, file), 'rb') as handle:\n",
    "            outlist_ = pickle.load(handle)\n",
    "           \n",
    "            for pid, o in enumerate(outlist_):\n",
    "                if pid in selected_pids:\n",
    "                    o['run'] = run\n",
    "                    o['pid'] = pid\n",
    "                    outlist.append(o)\n",
    "        run += 1\n",
    "\n",
    "len(outlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4b4b62d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['table', 'cell', 'instruction', 'input', 'question', 'output', 'pre_output_proba_topn', 'pre_output_proba_topk', 'pre_output_true_entropies', 'post_output_sequences', 'post_output_proba_topn', 'post_output_proba_topk', 'post_output_true_entropies', 'elapsed', 'run', 'pid'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlist[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e57d940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1beafeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlist[-1]['run']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dc9317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('644.data.pickle', 'wb') as handle:\n",
    "    pickle.dump(outlist, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "749ad873",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 43159720\r\n",
      "-rw-r--r--@  1 bono  staff    15M May 14  2024 turl_test_2k_prompts_50.jsonl\r\n",
      "-rw-r--r--   1 bono  staff    17B Feb 10 10:37 README.md\r\n",
      "-rwx------@  1 bono  staff   4.8K Feb 19 14:05 \u001b[31mrunner.py\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 bono  staff   137M Feb 19 16:32 gemma-2-2b-it.3892595837.pickle\r\n",
      "-rw-r--r--@  1 bono  staff   5.0K Feb 20 09:23 prompt100.py\r\n",
      "-rw-r--r--   1 bono  staff    16M Feb 20 20:07 tablellama.preprocessed.pickle\r\n",
      "-rw-r--r--@  1 bono  staff   5.2K Feb 20 21:04 prompt100tablellama.py\r\n",
      "-rw-r--r--   1 bono  staff   117M Feb 22 09:11 gemma-2-9b-it-4bit.3246737286.pickle\r\n",
      "drwxr-xr-x   4 bono  staff   128B Feb 27 07:56 \u001b[34mmirror\u001b[m\u001b[m\r\n",
      "drwxr-xr-x@ 11 bono  staff   352B Feb 27 08:19 \u001b[34m..\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 bono  staff   5.3K Mar  2 14:55 prompt100gemmalocal.py\r\n",
      "-rw-r--r--   1 bono  staff   222K Mar  4 13:37 jensension.ipynb\r\n",
      "drwxr-xr-x   4 bono  staff   128B Mar  5 20:03 \u001b[34molder tablellama\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  12 bono  staff   384B Mar  6 15:38 \u001b[34mtablellama10runs\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 bono  staff   450K Mar  6 16:03 tablellama-multipass.ipynb\r\n",
      "-rw-r--r--   1 bono  staff   450K Mar  6 16:09 tablellama-plot copy.ipynb\r\n",
      "drwxr-xr-x  16 bono  staff   512B Mar  6 17:50 \u001b[34m.git\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 bono  staff   6.2K Mar  6 19:31 prompt_wip.py\r\n",
      "-rw-rw----@  1 bono  staff   1.3M Mar  7 14:52 delme.xlsx\r\n",
      "-rw-r--r--   1 bono  staff   726K Mar  7 15:06 logits-tablellama.ipynb\r\n",
      "-rw-r--r--   1 bono  staff   779K Mar  7 15:41 tablellama-plot.ipynb\r\n",
      "-rw-r--r--@  1 bono  staff   4.4G Mar  8 13:37 TableLlama.2573653229.pickle\r\n",
      "drwxr-xr-x   2 bono  staff    64B Mar  8 14:54 \u001b[34mtest\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 bono  staff   8.0K Mar  8 14:54 .DS_Store\r\n",
      "-rw-r--r--@  1 bono  staff   4.4G Mar  9 10:21 TableLlama.1765523202.pickle\r\n",
      "-rw-r--r--@  1 bono  staff   4.4G Mar 10 08:18 TableLlama.940523022.pickle\r\n",
      "drwxr-xr-x   9 bono  staff   288B Mar 10 08:36 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 bono  staff   127M Mar 10 08:46 1800.pickle\r\n",
      "-rw-r--r--   1 bono  staff    33K Mar 10 09:00 orrors.ipynb\r\n",
      "-rw-r--r--   1 bono  staff    17K Mar 10 09:02 cohesion.ipynb\r\n",
      "drwxr-xr-x  32 bono  staff   1.0K Mar 10 09:02 \u001b[34m.\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 bono  staff   6.9G Mar 10 09:02 1800.data.pickle\r\n"
     ]
    }
   ],
   "source": [
    "!ls -larth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fbbdca",
   "metadata": {},
   "source": [
    "### DELETE ME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08531053",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './644sample/644.data.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./644sample/644.data.pickle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m      3\u001b[0m     selected \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(handle)\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './644sample/644.data.pickle'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('./644sample/644.data.pickle', 'rb') as handle:\n",
    "    selected = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ecdbbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2752"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ae2df27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2752"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "688*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3194b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.22 ms, sys: 1.83 ms, total: 5.05 ms\n",
      "Wall time: 5.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "selected_pids = set([s['pid'] for s in selected])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b627c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('selected_pids.688.pickle', 'wb') as handle:\n",
    "    pickle.dump(selected_pids, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98a006d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3810400\r\n",
      "-rw-r--r--@  1 bono  staff    15M May 14  2024 turl_test_2k_prompts_50.jsonl\r\n",
      "-rw-r--r--   1 bono  staff    17B Feb 10 10:37 README.md\r\n",
      "-rwx------@  1 bono  staff   4.8K Feb 19 14:05 \u001b[31mrunner.py\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 bono  staff   137M Feb 19 16:32 gemma-2-2b-it.3892595837.pickle\r\n",
      "-rw-r--r--@  1 bono  staff   5.0K Feb 20 09:23 prompt100.py\r\n",
      "-rw-r--r--   1 bono  staff    16M Feb 20 20:07 tablellama.preprocessed.pickle\r\n",
      "-rw-r--r--@  1 bono  staff   5.2K Feb 20 21:04 prompt100tablellama.py\r\n",
      "-rw-r--r--   1 bono  staff   117M Feb 22 09:11 gemma-2-9b-it-4bit.3246737286.pickle\r\n",
      "drwxr-xr-x   4 bono  staff   128B Feb 27 07:56 \u001b[34mmirror\u001b[m\u001b[m\r\n",
      "drwxr-xr-x@ 11 bono  staff   352B Feb 27 08:19 \u001b[34m..\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 bono  staff   5.3K Mar  2 14:55 prompt100gemmalocal.py\r\n",
      "-rw-r--r--   1 bono  staff   222K Mar  4 13:37 jensension.ipynb\r\n",
      "drwxr-xr-x   4 bono  staff   128B Mar  5 20:03 \u001b[34molder tablellama\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  12 bono  staff   384B Mar  6 15:38 \u001b[34mtablellama10runs\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 bono  staff   450K Mar  6 16:03 tablellama-multipass.ipynb\r\n",
      "-rw-r--r--   1 bono  staff   450K Mar  6 16:09 tablellama-plot copy.ipynb\r\n",
      "-rw-r--r--@  1 bono  staff   6.2K Mar  6 19:31 prompt_wip.py\r\n",
      "-rw-r--r--@  1 bono  staff   6.2K Mar  6 19:31 prompt_wip sel.py\r\n",
      "-rw-r--r--   1 bono  staff   726K Mar  7 15:06 logits-tablellama.ipynb\r\n",
      "drwxr-xr-x   2 bono  staff    64B Mar  8 14:54 \u001b[34mtest\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 bono  staff   127M Mar 10 08:46 1800.pickle\r\n",
      "-rw-rw----@  1 bono  staff   4.0M Mar 10 10:26 delme.xlsx\r\n",
      "drwxr-xr-x   9 bono  staff   288B Mar 10 16:41 \u001b[34mpdf\u001b[m\u001b[m\r\n",
      "drwxr-xr-x  10 bono  staff   320B Mar 11 09:07 \u001b[34m.ipynb_checkpoints\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 bono  staff   892K Mar 11 10:52 tablellama-classifier.ipynb\r\n",
      "drwxr-xr-x   4 bono  staff   128B Mar 11 15:24 \u001b[34m644sample\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 bono  staff   1.0G Mar 11 15:27 preprocessed.pickle\r\n",
      "-rw-r--r--   1 bono  staff   1.4M Mar 11 15:28 tablellama-plot.ipynb\r\n",
      "-rw-r--r--   1 bono  staff   390M Mar 11 16:16 cohesion.pickle\r\n",
      "-rw-r--r--   1 bono  staff   114K Mar 11 16:25 cohesion.ipynb\r\n",
      "drwxr-xr-x  16 bono  staff   512B Mar 11 16:36 \u001b[34m.git\u001b[m\u001b[m\r\n",
      "-rw-r--r--@  1 bono  staff    10K Mar 11 17:49 .DS_Store\r\n",
      "-rw-r--r--   1 bono  staff     0B Mar 11 17:52 selected_pids.644.pickle\r\n",
      "-rw-r--r--   1 bono  staff    36K Mar 11 17:56 select_errors_from_1800.ipynb\r\n",
      "drwxr-xr-x  36 bono  staff   1.1K Mar 11 17:56 \u001b[34m.\u001b[m\u001b[m\r\n",
      "-rw-r--r--   1 bono  staff   1.9K Mar 11 18:12 selected_pids.688.pickle\r\n"
     ]
    }
   ],
   "source": [
    "!ls -larth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297c3fc",
   "metadata": {},
   "source": [
    "### debug (transition scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fa3d0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/huggingface/transformers/issues/24841"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7115aaa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186700fb9f2e4f3e835d1e3c89f7c1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "\n",
    "# run params\n",
    "temperature=1.0\n",
    "top_p=0.9\n",
    "max_new_tokens=64\n",
    "use_cache=True\n",
    "device = \"mps\"\n",
    "\n",
    "model_name = \"osunlp/TableLlama\"\n",
    "\n",
    "# load inputs\n",
    "file_path = \"turl_test_2k_prompts_50.jsonl\"\n",
    "device = torch.device(device)\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    prompts = [json.loads(line) for line in f]\n",
    "\n",
    "config = transformers.AutoConfig.from_pretrained(model_name)\n",
    "orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(device)\n",
    "model.resize_token_embeddings(32001)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=orig_ctx_len, padding_side=\"left\", use_fast=False)\n",
    "model.eval()\n",
    "\n",
    "# build prompts\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_seg}\\n\\n### Question:\\n{question}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def generate_prompt(instruction, question, input_seg=None):\n",
    "    question += \" Answer with just a candidate, selected from the provided referent entity candidates list, and nothing else. The selected candidate must be reported verbatim from the list provided as input. Each candidate in the list is enclosed between < and > and reports [DESC] and [TYPE] information.\"\n",
    "    if input_seg:\n",
    "        return PROMPT_DICT[\"prompt_input\"].format(instruction=instruction, input_seg=input_seg, question=question)\n",
    "    else:\n",
    "        return PROMPT_DICT[\"prompt_no_input\"].format(instruction=instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6fc7b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = prompts[742] # 742 is shortest\n",
    "p = prompts[840] # average joe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3384e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.6 ms, sys: 920 ms, total: 937 ms\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "from transformers import StaticCache\n",
    "\n",
    "# Initialize prompt cache\n",
    "prompt_cache = StaticCache(config=model.config, max_batch_size=1, max_cache_len=2**14, device='mps', dtype=torch.float16)\n",
    "\n",
    "# Generate initial prompt input\n",
    "prompt = generate_prompt(p[\"instruction\"], p[\"question\"], p[\"input\"])\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "prompt_ = prompt[:-1]\n",
    "inputs_ = tokenizer(prompt_, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d24d7695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 201 ms, sys: 614 ms, total: 815 ms\n",
      "Wall time: 2.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# as it were in the script\n",
    "with torch.no_grad():\n",
    "    pre_output = model(**inputs, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd88273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.81 s, sys: 1.15 s, total: 2.95 s\n",
      "Wall time: 5.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# as it were in the script\n",
    "\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "448cbd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.3 s, sys: 6.45 s, total: 10.7 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# No cache\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    use_cache=False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e295f0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e88b9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 128 ms, sys: 656 ms, total: 784 ms\n",
      "Wall time: 4.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt_cache = StaticCache(config=model.config, max_batch_size=1, \n",
    "                           max_cache_len = 2**12, \n",
    "                           device='mps', dtype=torch.bfloat16)\n",
    "\n",
    "# Generate cache\n",
    "with torch.no_grad():\n",
    "    pre_output = model(**inputs_, past_key_values=prompt_cache, use_cache=False)\n",
    "    prompt_cache = pre_output.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cffa4346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.32 s, sys: 905 ms, total: 2.23 s\n",
      "Wall time: 6.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# With cache\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    past_key_values=copy.deepcopy(prompt_cache),\n",
    "    use_cache=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08e74486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.39 s, sys: 5.9 s, total: 9.3 s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# With cache\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    past_key_values=copy.deepcopy(prompt_cache),\n",
    "    use_cache=False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5846f85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.45 s, sys: 1.54 s, total: 2.99 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Reset cache\n",
    "\n",
    "prompt_cache = StaticCache(config=model.config, max_batch_size=1, \n",
    "                           max_cache_len = 2**12, \n",
    "                           device='mps', dtype=torch.bfloat16)\n",
    "\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    past_key_values=copy.deepcopy(prompt_cache),\n",
    "    use_cache=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e81024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39761220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1aa56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01b676a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05123688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.41 s, sys: 7.7 s, total: 9.11 s\n",
      "Wall time: 24.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# With cache\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    past_key_values=copy.deepcopy(prompt_cache),\n",
    "    use_cache=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6ee1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbce1c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5854033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 308 ms, sys: 650 ms, total: 958 ms\n",
      "Wall time: 5.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = generate_prompt(p[\"instruction\"], p[\"question\"], p[\"input\"])\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    pre_output = model(**inputs, use_cache=use_cache)\n",
    "pre_output = pre_output.logits.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a178eeb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1073])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e91459b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16384"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5379a3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ede7d98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 199 ms, sys: 1.16 s, total: 1.36 s\n",
      "Wall time: 9.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import copy\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, StaticCache\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "\n",
    "prompt_cache = StaticCache(config=model.config, max_batch_size=1, max_cache_len=2**14, device='mps', dtype=torch.float16)\n",
    "\n",
    "prompt = generate_prompt(p[\"instruction\"], p[\"question\"], p[\"input\"])\n",
    "prompt_ = prompt[:-1]\n",
    "\n",
    "inputs = tokenizer(prompt_, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pre_output = model(**inputs, past_key_values = prompt_cache)\n",
    "    prompt_cache = pre_output.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c48e6190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 98.4 ms, sys: 367 ms, total: 466 ms\n",
      "Wall time: 3.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_inputs = tokenizer(prompt, return_tensors=\"pt\").to('mps')\n",
    "outputs = model.generate(**new_inputs, past_key_values=prompt_cache, max_new_tokens=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7fcd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540fc108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "089de13b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 81.38 GB, other allocations: 1.84 MB, max allowed: 81.60 GB). Tried to allocate 512.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:5\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/cache_utils.py:1172\u001b[0m, in \u001b[0;36mStaticCache.__init__\u001b[0;34m(self, config, batch_size, max_cache_len, device, dtype, max_batch_size, layer_device_map)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;66;03m# Notes:\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;66;03m# 1. `mark_static_address` is used to tag the cache as an fixed data pointer, preventing cuda graph\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;66;03m#     breaks when updating the cache. It can't be used if the cache code is being compiled (but in that case\u001b[39;00m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;66;03m#     it is not needed anyway)\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;66;03m# 2. `torch.export()` requires mutations to be registered as buffers.\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m-> 1172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey_cache_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_device\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue_cache_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mzeros(cache_shape, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mlayer_device))\n\u001b[1;32m   1174\u001b[0m     new_layer_key_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey_cache_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 81.38 GB, other allocations: 1.84 MB, max allowed: 81.60 GB). Tried to allocate 512.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf673fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd950a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bf322a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.65 s, sys: 6.02 s, total: 7.67 s\n",
      "Wall time: 29.3 s\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bb0e4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "132d8a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 13866,   338,  ..., 29937, 13291, 29901]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46db297d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n"
     ]
    }
   ],
   "source": [
    "    num_tokens=20\n",
    "    \n",
    "    outputs = model(**inputs, use_cache=True, return_dict=True)\n",
    "    past = outputs.past_key_values\n",
    "\n",
    "    generated = input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "753035c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 81.61 GB, other allocations: 3.81 MB, max allowed: 81.60 GB). Tried to allocate 8.64 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_tokens):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# For each new step, only pass the last token along with the cache\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     next_input \u001b[38;5;241m=\u001b[39m generated[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m----> 6\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m      8\u001b[0m     past \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpast_key_values  \u001b[38;5;66;03m# Update the cache with new key/value pairs\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    935\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m         position_embeddings,\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:676\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    675\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 676\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    689\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:602\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[1;32m    600\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m causal_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m q_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    611\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    612\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 81.61 GB, other allocations: 3.81 MB, max allowed: 81.60 GB). Tried to allocate 8.64 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "    for k in range(5):\n",
    "        print(k)\n",
    "        for _ in range(num_tokens):\n",
    "            # For each new step, only pass the last token along with the cache\n",
    "            next_input = generated[:, -1:]\n",
    "            outputs = model(next_input, past_key_values=past, use_cache=True, return_dict=True)\n",
    "            logits = outputs.logits\n",
    "            past = outputs.past_key_values  # Update the cache with new key/value pairs\n",
    "\n",
    "            # Greedily select the next token (alternatively, you can sample)\n",
    "            next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            generated = torch.cat((generated, next_token), dim=-1)\n",
    "\n",
    "        # Decode the full sequence of tokens into text\n",
    "        pippo = tokenizer.decode(generated[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6837420a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06964fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbeec7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_key_values = copy.deepcopy(prompt_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc0c6bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_compiled_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'batch_size',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_max_cache_shape',\n",
       " 'get_max_length',\n",
       " 'get_parameter',\n",
       " 'get_seq_length',\n",
       " 'get_submodule',\n",
       " 'get_usable_length',\n",
       " 'half',\n",
       " 'head_dim',\n",
       " 'ipu',\n",
       " 'key_cache',\n",
       " 'key_cache_0',\n",
       " 'key_cache_1',\n",
       " 'key_cache_10',\n",
       " 'key_cache_11',\n",
       " 'key_cache_12',\n",
       " 'key_cache_13',\n",
       " 'key_cache_14',\n",
       " 'key_cache_15',\n",
       " 'key_cache_16',\n",
       " 'key_cache_17',\n",
       " 'key_cache_18',\n",
       " 'key_cache_19',\n",
       " 'key_cache_2',\n",
       " 'key_cache_20',\n",
       " 'key_cache_21',\n",
       " 'key_cache_22',\n",
       " 'key_cache_23',\n",
       " 'key_cache_24',\n",
       " 'key_cache_25',\n",
       " 'key_cache_26',\n",
       " 'key_cache_27',\n",
       " 'key_cache_28',\n",
       " 'key_cache_29',\n",
       " 'key_cache_3',\n",
       " 'key_cache_30',\n",
       " 'key_cache_31',\n",
       " 'key_cache_4',\n",
       " 'key_cache_5',\n",
       " 'key_cache_6',\n",
       " 'key_cache_7',\n",
       " 'key_cache_8',\n",
       " 'key_cache_9',\n",
       " 'load_state_dict',\n",
       " 'max_cache_len',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'num_key_value_heads',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'reorder_cache',\n",
       " 'requires_grad_',\n",
       " 'reset',\n",
       " 'seen_tokens',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'update',\n",
       " 'value_cache',\n",
       " 'value_cache_0',\n",
       " 'value_cache_1',\n",
       " 'value_cache_10',\n",
       " 'value_cache_11',\n",
       " 'value_cache_12',\n",
       " 'value_cache_13',\n",
       " 'value_cache_14',\n",
       " 'value_cache_15',\n",
       " 'value_cache_16',\n",
       " 'value_cache_17',\n",
       " 'value_cache_18',\n",
       " 'value_cache_19',\n",
       " 'value_cache_2',\n",
       " 'value_cache_20',\n",
       " 'value_cache_21',\n",
       " 'value_cache_22',\n",
       " 'value_cache_23',\n",
       " 'value_cache_24',\n",
       " 'value_cache_25',\n",
       " 'value_cache_26',\n",
       " 'value_cache_27',\n",
       " 'value_cache_28',\n",
       " 'value_cache_29',\n",
       " 'value_cache_3',\n",
       " 'value_cache_30',\n",
       " 'value_cache_31',\n",
       " 'value_cache_4',\n",
       " 'value_cache_5',\n",
       " 'value_cache_6',\n",
       " 'value_cache_7',\n",
       " 'value_cache_8',\n",
       " 'value_cache_9',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(past_key_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc4c0cdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index -1 is out of bounds for dimension 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# new_inputs = tokenizer(prompt + \" fottiti\", return_tensors=\"pt\").to('mps')\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/generation/utils.py:3199\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3193\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_initial_cache_position(input_ids, model_kwargs)\n\u001b[1;32m   3195\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_unfinished_sequences(\n\u001b[1;32m   3196\u001b[0m     this_peer_finished, synced_gpus, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice, cur_len\u001b[38;5;241m=\u001b[39mcur_len, max_length\u001b[38;5;241m=\u001b[39mmax_length\n\u001b[1;32m   3197\u001b[0m ):\n\u001b[1;32m   3198\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[0;32m-> 3199\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_inputs_for_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3201\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n\u001b[1;32m   3202\u001b[0m     model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n",
      "File \u001b[0;32m~/miniforge3/envs/doubt/lib/python3.8/site-packages/transformers/generation/utils.py:384\u001b[0m, in \u001b[0;36mGenerationMixin.prepare_inputs_for_generation\u001b[0;34m(self, input_ids, past_key_values, attention_mask, inputs_embeds, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m past_key_values\n\u001b[0;32m--> 384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mcache_position\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:  \u001b[38;5;66;03m# Exception 1 or Exception 3\u001b[39;00m\n\u001b[1;32m    385\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m input_ids[:, \u001b[38;5;241m-\u001b[39mcache_position\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] :]\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m cache_position\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:  \u001b[38;5;66;03m# Default case (the \"else\", a no op, is Exception 2)\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index -1 is out of bounds for dimension 0 with size 0"
     ]
    }
   ],
   "source": [
    "# new_inputs = tokenizer(prompt + \" fottiti\", return_tensors=\"pt\").to('mps')\n",
    "outputs = model.generate(**inputs, past_key_values=past_key_values, max_new_tokens=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5981ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def generate_text_with_cache(model, tokenizer, prompt, num_tokens=20):\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    outputs = model(input_ids, use_cache=True, return_dict=True)\n",
    "    past = outputs.past_key_values  # Cache of previous key/value pairs\n",
    "\n",
    "    generated = input_ids\n",
    "\n",
    "    # Generate tokens one by one using the cached past\n",
    "    for _ in range(num_tokens):\n",
    "        # For each new step, only pass the last token along with the cache\n",
    "        next_input = generated[:, -1:]\n",
    "        outputs = model(next_input, past_key_values=past, use_cache=True, return_dict=True)\n",
    "        logits = outputs.logits\n",
    "        past = outputs.past_key_values  # Update the cache with new key/value pairs\n",
    "\n",
    "        # Greedily select the next token (alternatively, you can sample)\n",
    "        next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        generated = torch.cat((generated, next_token), dim=-1)\n",
    "\n",
    "    # Decode the full sequence of tokens into text\n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the model you want to use; for example, GPT-2\n",
    "    model_name = \"gpt2\"  # Change this to your desired model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    prompt = \"Hello, how are you?\"\n",
    "    print(\"Prompt:\", prompt)\n",
    "    \n",
    "    # Generate text by extending the prompt with additional tokens\n",
    "    output_text = generate_text_with_cache(model, tokenizer, prompt, num_tokens=20)\n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8258091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c4a0cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e75ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, StaticCache\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "\n",
    "DEVICE, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Init StaticCache with big enough max-length (1024 tokens for the below example)\n",
    "# You can also init a DynamicCache, if that suits you better\n",
    "prompt_cache = StaticCache(config=model.config, max_batch_size=1, max_cache_len=1024, device=DEVICE, dtype=torch.bfloat16)\n",
    "\n",
    "INITIAL_PROMPT = \"You are a helpful assistant. \"\n",
    "inputs_initial_prompt = tokenizer(INITIAL_PROMPT, return_tensors=\"pt\").to(DEVICE)\n",
    "# This is the common prompt cached, we need to run forward without grad to be abel to copy\n",
    "with torch.no_grad():\n",
    "     prompt_cache = model(**inputs_initial_prompt, past_key_values = prompt_cache).past_key_values\n",
    "\n",
    "prompts = [\"Help me to write a blogpost about travelling.\", \"What is the capital of France?\"]\n",
    "responses = []\n",
    "for prompt in prompts:\n",
    "    new_inputs = tokenizer(INITIAL_PROMPT + prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    past_key_values = copy.deepcopy(prompt_cache)\n",
    "    outputs = model.generate(**new_inputs, past_key_values=past_key_values,max_new_tokens=20)\n",
    "    response = tokenizer.batch_decode(outputs)[0]\n",
    "    responses.append(response)\n",
    "\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb64b717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.87 s, sys: 945 ms, total: 2.82 s\n",
      "Wall time: 5.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    use_cache=use_cache\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "539779e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_scores = model.compute_transition_scores(post_output.sequences, post_output.scores, normalize_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "772738ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1094]), 21, torch.Size([1, 32001]), torch.Size([1, 21]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_output.sequences.shape, len(post_output.scores), post_output.scores[0].shape, transition_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72a9695f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32001])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_output.scores[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e0d2817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7f687ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_likelihoods = [score.item() for score in transition_scores[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5af0080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3557f69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_scores_s = model.compute_transition_scores(post_output.sequences, post_output.scores, normalize_logits=True)\n",
    "log_likelihoods_s = [score.item() for score in transition_scores[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e079b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f264bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-4.768370445162873e-07,\n",
       " -0.09085541218519211,\n",
       " -0.10185058414936066,\n",
       " -4.768370445162873e-07,\n",
       " -1.1920928244535389e-07,\n",
       " -2.407998726994265e-05,\n",
       " -0.0023836076725274324,\n",
       " 0.0,\n",
       " 0.0,\n",
       " -0.022649873048067093,\n",
       " -0.007525428663939238,\n",
       " -9.536738616588991e-07,\n",
       " -3.576278118089249e-07,\n",
       " 0.0,\n",
       " -2.3841855067985307e-07,\n",
       " 0.0,\n",
       " -3.2186455882765586e-06,\n",
       " -7.867782187531702e-06,\n",
       " -5.960462772236497e-07,\n",
       " -1.1920928244535389e-07,\n",
       " 0.0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_scores_l = model.compute_transition_scores(post_output.sequences, post_output.logits, normalize_logits=True)\n",
    "log_likelihoods_l = [score.item() for score in transition_scores[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91fad576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.7684e-07, -9.0855e-02, -1.0185e-01, -4.7684e-07, -1.1921e-07,\n",
       "         -2.4080e-05, -2.3836e-03,  0.0000e+00,  0.0000e+00, -2.2650e-02,\n",
       "         -7.5254e-03, -9.5367e-07, -3.5763e-07,  0.0000e+00, -2.3842e-07,\n",
       "          0.0000e+00, -3.2186e-06, -7.8678e-06, -5.9605e-07, -1.1921e-07,\n",
       "          0.0000e+00]], device='mps:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2d91d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doubt",
   "language": "python",
   "name": "doubt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
