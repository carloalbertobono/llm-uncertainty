{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fa3d0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/huggingface/transformers/issues/24841"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7115aaa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186700fb9f2e4f3e835d1e3c89f7c1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "\n",
    "# run params\n",
    "temperature=1.0\n",
    "top_p=0.9\n",
    "max_new_tokens=64\n",
    "use_cache=True\n",
    "device = \"mps\"\n",
    "\n",
    "model_name = \"osunlp/TableLlama\"\n",
    "\n",
    "# load inputs\n",
    "file_path = \"turl_test_2k_prompts_50.jsonl\"\n",
    "device = torch.device(device)\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    prompts = [json.loads(line) for line in f]\n",
    "\n",
    "config = transformers.AutoConfig.from_pretrained(model_name)\n",
    "orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(device)\n",
    "model.resize_token_embeddings(32001)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=orig_ctx_len, padding_side=\"left\", use_fast=False)\n",
    "model.eval()\n",
    "\n",
    "# build prompts\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_seg}\\n\\n### Question:\\n{question}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def generate_prompt(instruction, question, input_seg=None):\n",
    "    question += \" Answer with just a candidate, selected from the provided referent entity candidates list, and nothing else. The selected candidate must be reported verbatim from the list provided as input. Each candidate in the list is enclosed between < and > and reports [DESC] and [TYPE] information.\"\n",
    "    if input_seg:\n",
    "        return PROMPT_DICT[\"prompt_input\"].format(instruction=instruction, input_seg=input_seg, question=question)\n",
    "    else:\n",
    "        return PROMPT_DICT[\"prompt_no_input\"].format(instruction=instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6fc7b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = prompts[742] # 742 is shortest\n",
    "p = prompts[840] # average joe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3384e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.6 ms, sys: 920 ms, total: 937 ms\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import copy\n",
    "import torch\n",
    "from transformers import StaticCache\n",
    "\n",
    "# Initialize prompt cache\n",
    "prompt_cache = StaticCache(config=model.config, max_batch_size=1, max_cache_len=2**14, device='mps', dtype=torch.float16)\n",
    "\n",
    "# Generate initial prompt input\n",
    "prompt = generate_prompt(p[\"instruction\"], p[\"question\"], p[\"input\"])\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "prompt_ = prompt[:-1]\n",
    "inputs_ = tokenizer(prompt_, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d24d7695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 201 ms, sys: 614 ms, total: 815 ms\n",
      "Wall time: 2.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# as it were in the script\n",
    "with torch.no_grad():\n",
    "    pre_output = model(**inputs, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd88273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.81 s, sys: 1.15 s, total: 2.95 s\n",
      "Wall time: 5.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# as it were in the script\n",
    "\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "448cbd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.3 s, sys: 6.45 s, total: 10.7 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# No cache\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    use_cache=False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e295f0f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e88b9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 128 ms, sys: 656 ms, total: 784 ms\n",
      "Wall time: 4.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt_cache = StaticCache(config=model.config, max_batch_size=1, \n",
    "                           max_cache_len = 2**12, \n",
    "                           device='mps', dtype=torch.bfloat16)\n",
    "\n",
    "# Generate cache\n",
    "with torch.no_grad():\n",
    "    pre_output = model(**inputs_, past_key_values=prompt_cache, use_cache=False)\n",
    "    prompt_cache = pre_output.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cffa4346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.32 s, sys: 905 ms, total: 2.23 s\n",
      "Wall time: 6.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# With cache\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    past_key_values=copy.deepcopy(prompt_cache),\n",
    "    use_cache=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08e74486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.39 s, sys: 5.9 s, total: 9.3 s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# With cache\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    past_key_values=copy.deepcopy(prompt_cache),\n",
    "    use_cache=False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5846f85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.45 s, sys: 1.54 s, total: 2.99 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# Reset cache\n",
    "\n",
    "prompt_cache = StaticCache(config=model.config, max_batch_size=1, \n",
    "                           max_cache_len = 2**12, \n",
    "                           device='mps', dtype=torch.bfloat16)\n",
    "\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    past_key_values=copy.deepcopy(prompt_cache),\n",
    "    use_cache=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e81024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39761220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1aa56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01b676a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05123688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.41 s, sys: 7.7 s, total: 9.11 s\n",
      "Wall time: 24.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# With cache\n",
    "post_output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_logits=True,\n",
    "    past_key_values=copy.deepcopy(prompt_cache),\n",
    "    use_cache=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6ee1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbce1c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5854033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 308 ms, sys: 650 ms, total: 958 ms\n",
      "Wall time: 5.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = generate_prompt(p[\"instruction\"], p[\"question\"], p[\"input\"])\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    pre_output = model(**inputs, use_cache=use_cache)\n",
    "pre_output = pre_output.logits.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a178eeb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1073])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e91459b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16384"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5379a3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "':'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ede7d98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 199 ms, sys: 1.16 s, total: 1.36 s\n",
      "Wall time: 9.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import copy\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, StaticCache\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "\n",
    "prompt_cache = StaticCache(config=model.config, max_batch_size=1, max_cache_len=2**14, device='mps', dtype=torch.float16)\n",
    "\n",
    "prompt = generate_prompt(p[\"instruction\"], p[\"question\"], p[\"input\"])\n",
    "prompt_ = prompt[:-1]\n",
    "\n",
    "inputs = tokenizer(prompt_, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pre_output = model(**inputs, past_key_values = prompt_cache)\n",
    "    prompt_cache = pre_output.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c48e6190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 98.4 ms, sys: 367 ms, total: 466 ms\n",
      "Wall time: 3.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_inputs = tokenizer(prompt, return_tensors=\"pt\").to('mps')\n",
    "outputs = model.generate(**new_inputs, past_key_values=prompt_cache, max_new_tokens=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doubt",
   "language": "python",
   "name": "doubt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
